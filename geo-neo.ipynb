{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f691eb20-99ad-44c2-a384-0376451cd99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0000, 3.9900, 3.9900,  ..., 3.8700, 3.8700, 3.8700],\n",
       "        [3.8700, 3.8700, 3.8700,  ..., 3.7500, 3.7500, 3.7500],\n",
       "        [3.7500, 3.7500, 3.7500,  ..., 3.6300, 3.6300, 3.6300],\n",
       "        ...,\n",
       "        [3.6300, 3.6300, 3.6300,  ..., 3.7500, 3.7500, 3.7500],\n",
       "        [3.7500, 3.7500, 3.7500,  ..., 3.8700, 3.8700, 3.8700],\n",
       "        [3.8700, 3.8700, 3.8700,  ..., 3.9900, 3.9900, 4.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def truncate(number, decimals=0):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor\n",
    "\n",
    "def generate_seq_embed(func,                       \n",
    "                       seq_len = 128,\n",
    "                       range_start = -2,\n",
    "                       range_end = 2,\n",
    "                       lower_bound = -2000,\n",
    "                       upper_bound = 2000, \n",
    "                       embed_size = 768):\n",
    "    # start = time.time()\n",
    "    seq = torch.zeros(seq_len, embed_size)\n",
    "    step = (range_end - range_start) / (seq_len*embed_size - 1)\n",
    "    for i in range(seq_len*embed_size):\n",
    "        x = range_start + i * step\n",
    "        #print(formula.replace('x', str(x)))\n",
    "        #y = eval(formula.replace('x', str(x)))\n",
    "        y = func(x)\n",
    "        y = max(lower_bound, min(upper_bound,y))\n",
    "        trunc_digits = max(0, 2 - math.floor(math.log(abs(y), 10)))\n",
    "        seq[i // embed_size][i % embed_size] = truncate(y, trunc_digits)\n",
    "    # end = time.time()\n",
    "    # print(end - start, \"seconds\")\n",
    "    return seq\n",
    "generate_seq_embed(eval(\"lambda x: x**2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0330a445-f7e0-4080-88ea-6fcb7055c9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad92bafa7af948db950e0ab658a00050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5c3f3e878641e388b49125401aecf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95df562372c64591b1089a2e2a6ca139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['a'],\n",
       "        num_rows: 118\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['a'],\n",
       "        num_rows: 19963\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: https://huggingface.co/docs/transformers/v4.17.0/en/tasks/language_modeling\n",
    "\n",
    "import datasets\n",
    "\n",
    "eli5 = datasets.load_dataset(\"/home/mcwave/data/textbooks/eqs_withcoords_test\")\n",
    "# eli5 = eli5.train_test_split(test_size=0.2)\n",
    "\n",
    "#\n",
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"] = datasets.load_dataset(\"/home/mcwave/data/textbooks/eqs_withcoords\")[\"train\"]\n",
    "eli5[\"test\"] = datasets.load_dataset(\"/home/mcwave/data/textbooks/eqs_withcoords_test\")[\"test\"]\n",
    "eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1658e10b-ab88-408e-935c-47e779423537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cd3adfe1bb49f2a4e4e0ae6dd3e33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19963 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6730ec7a4a194d188aedc7d950d4716a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19963 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', \"bos_token\": \"[BOS]\"})\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.add_special_tokens({'bos_token': '[BOS]'})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"a\"]], padding=True, truncation=True)\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "def add_labels(examples):\n",
    "    # print(len(examples[\"input_ids\"]))\n",
    "    examples[\"input_ids\"] += [50257] * (128 - len(examples[\"input_ids\"]))\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    # while len(examples[\"labels\"]) < 128:\n",
    "    #     examples[\"labels\"].append(50257)\n",
    "    return examples\n",
    "\n",
    "lm_dataset = tokenized_eli5.map(add_labels, batched=False, num_proc=1)\n",
    "# lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)\n",
    "# lm_dataset[\"train\"][\"labels\"][0]\n",
    "# for i in range(len(lm_dataset[\"train\"][\"attention_mask\"])):\n",
    "#     for j in range(len(lm_dataset[\"train\"][\"attention_mask\"][i])):\n",
    "#         lm_dataset[\"train\"][\"attention_mask\"][i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500c57b-7766-47ba-bed4-3d3fff2b46be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data = np.load(\"/home/mcwave/data/textbooks/eqs_embeds.npy\")\n",
    "data_list = []\n",
    "from datasets import Dataset\n",
    "# len(lm_dataset[\"train\"][\"input_ids\"])\n",
    "for i in range(len(lm_dataset[\"train\"])):\n",
    "    try:\n",
    "        # print(\"good\")\n",
    "        # flattened = torch.flatten(torch.Tensor(data[i]))\n",
    "        flattened = []\n",
    "        # print(torch.Tensor(data[i]).shape)\n",
    "        # print(flattened.shape)\n",
    "        if flattened.shape[0] != 98304:\n",
    "            # print(\"good2\")\n",
    "            data_list.append(torch.Tensor.tolist(torch.ones([98304])))\n",
    "        else:\n",
    "            data_list.append(torch.Tensor.tolist(flattened))\n",
    "            continue\n",
    "    except:\n",
    "        print(\"bad\")\n",
    "        data_list.append(torch.Tensor.tolist(torch.ones([98304])))\n",
    "\n",
    "data_test = np.load(\"/home/mcwave/data/textbooks/eqs_embeds_test.npy\")\n",
    "data_list_test = []\n",
    "for i in range(len(lm_dataset[\"test\"])):\n",
    "    try:\n",
    "        # flattened = torch.flatten(torch.Tensor(data_test[i]))\n",
    "        flattened = []\n",
    "        # print(torch.Tensor(data[i]).shape)\n",
    "        # print(flattened.shape)\n",
    "        if flattened.shape[0] != 98304:\n",
    "            data_list_test.append(torch.Tensor.tolist(torch.ones([98304])))\n",
    "        else:\n",
    "            data_list_test.append(torch.Tensor.tolist(flattened))\n",
    "            continue\n",
    "    except:\n",
    "        data_list_test.append(torch.Tensor.tolist(torch.ones([98304])))\n",
    "print(\"Loaded data\")\n",
    "# dataset_2 = Dataset.from_dict({\"q\": data_list})\n",
    "# dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0786363d-b52f-420e-815b-27560170ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_dataset['train'] = lm_dataset['train'].add_column(\"inputs_embeds\", data_list)\n",
    "lm_dataset['test'] = lm_dataset['test'].add_column(\"inputs_embeds\", data_list_test)\n",
    "# lm_dataset[\"train\"].save_to_disk(\"train_dataset_2.hf\")\n",
    "# lm_dataset[\"test\"].save_to_disk(\"test_dataset_2.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e404e37-a426-4e44-acf2-db300cc7bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "#pad 128 tokens in front and one BOS token\n",
    "\n",
    "def add_padding(val):\n",
    "    pad_and_bos_token = torch.cat((torch.full((1, 128-len(val[\"input_ids\"])), 50257)[0], torch.tensor([50258])))\n",
    "    \n",
    "    val[\"input_ids\"] = torch.Tensor.tolist(torch.cat((pad_and_bos_token, torch.Tensor(val[\"input_ids\"]))))\n",
    "    val[\"attention_mask\"] = torch.Tensor.tolist(torch.cat((torch.full((1, 129-len(val[\"attention_mask\"])), 0)[0], torch.Tensor(val[\"attention_mask\"]))))\n",
    "    return val\n",
    "\n",
    "selected = []\n",
    "for i in range(110):\n",
    "    selected.append(i)\n",
    "# lm_dataset[\"train\"] = datasets.load_from_disk(\"train_dataset_2.hf\").map(add_padding)\n",
    "# lm_dataset[\"test\"] = datasets.load_from_disk(\"test_dataset_2.hf\").select(selected).map(add_padding)\n",
    "lm_dataset[\"test\"] = lm_dataset[\"test\"].map(add_padding)\n",
    "# for i in lm_dataset[\"train\"][\"attention_mask\"]:\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00545af3-4bb8-44e7-be38-b34cc43abd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dataset[\"test\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5004b9e3-5d1e-42fa-a5a1-79513d7c6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "import numpy as np\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53b9af4-4192-4d55-ba88-f7db91a77d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-01 21:11:18,604] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50259. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "from transformers.models.gpt_neo.configuration_gpt_neo import GPTNeoConfig\n",
    "from transformers.models.gpt_neo.modeling_gpt_neo import *\n",
    "\n",
    "class MyGPTNeoForCausalLM(GPTNeoPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPTNeoModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past_key_values:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"position_ids\": position_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        # inputs_embeds = torch.ones([1,128,768]) #<-set inference inputs here\n",
    "        # print(\"forward\")\n",
    "        # print(inputs_embeds)\n",
    "        # print(input_ids)\n",
    "        # if len(inputs_embeds.shape) != 3:\n",
    "        #     # print(inputs_embeds.shape)\n",
    "        #     batch_size = inputs_embeds.shape[0]*inputs_embeds.shape[1]//98304\n",
    "        #     # print(inputs_embeds)\n",
    "        #     inputs_embeds = torch.reshape(inputs_embeds, (batch_size, 128, 768))\n",
    "        #     # print(inputs_embeds)\n",
    "        #     print(input_ids)\n",
    "        #     eq_embeds = self.transformer.wte(input_ids)[:,128:]\n",
    "        #     # print(eq_embeds.shape)\n",
    "        #     inputs_embeds = torch.cat((inputs_embeds, eq_embeds), 1)\n",
    "        #     # print(attention_mask.shape)\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,  \n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=None,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            # Compute loss in fp32 to match with mesh-tf version\n",
    "            # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n",
    "            lm_logits = lm_logits.to(torch.float32)\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            lm_logits = lm_logits.to(hidden_states.dtype)\n",
    "            loss = loss.to(hidden_states.dtype)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(\n",
    "        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n",
    "    ) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n",
    "        [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n",
    "        beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past_key_values\n",
    "        )\n",
    "\n",
    "model = MyGPTNeoForCausalLM.from_pretrained(\"/home/mcwave/code/word_problem_magnifier/results/2/checkpoint-1000\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b9b4e-da59-4b63-8ae6-227e14392a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset # pass in tokenized string, decode and turn into embeds in forward\n",
    "#debug lines: 1842, 2718\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results/2/3\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    num_train_epochs = 50, \n",
    "    per_device_train_batch_size = 10, \n",
    "    per_device_eval_batch_size = 10,\n",
    "    logging_steps = 1000,\n",
    "    eval_steps = 1000,\n",
    "    save_steps = 1000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = lm_dataset[\"test\"],\n",
    "    eval_dataset = lm_dataset[\"test\"],\n",
    "    data_collator = data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"/home/mcwave/code/word_problem_magnifier/results/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f7c72-b59a-4cad-a916-06a0fe252a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Pipeline,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "import torch\n",
    "from training.generate import generate_response\n",
    "\n",
    "model_path = \"/home/mcwave/code/word_problem_magnifier/results/2/checkpoint-1000\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "model = MyGPTNeoForCausalLM.from_pretrained(\n",
    "    model_path\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39418e86-ea29-4443-a6fc-cee96ed7a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = model.generate(\n",
    "    torch.Tensor([add_padding(tokenizer(\"x\"))[\"input_ids\"]]).type(torch.int64),\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]#utils 2760, 2771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d461de-f33a-4ab2-b5a0-e3d9dcc88992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \" \\\n",
    "#          \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \" \\\n",
    "#          \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    "\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=100,)\n",
    "# gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "# gen_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
