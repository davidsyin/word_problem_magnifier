{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2213e048-3bc7-4d46-8454-80a24f7b726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4f0837-8f86-4e0a-b32b-2840abf18dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"/home/mcwave/models/huggyllama-7b/llama-7b\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=False,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3faaba0-a038-491c-a955-cefe59d61e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/mcwave/anaconda3/envs/torch200 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('code/word_problem_magnifier/a47d31b6-0c8f-48e2-afa2-3ff8ccd55c34')}\n",
      "  warn(msg)\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/org/gnome/Terminal/screen/d2d21fc5_4f96_4acf_9218_97701abf63fa')}\n",
      "  warn(msg)\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/1862,unix/mcwave2'), PosixPath('local/mcwave2')}\n",
      "  warn(msg)\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n",
      "Key list: ['', 'model', 'model.embed_tokens', 'model.layers', 'model.layers.0', 'model.layers.0.self_attn', 'model.layers.0.self_attn.q_proj', 'model.layers.0.self_attn.k_proj', 'model.layers.0.self_attn.v_proj', 'model.layers.0.self_attn.o_proj', 'model.layers.0.self_attn.rotary_emb', 'model.layers.0.mlp', 'model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.down_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.act_fn', 'model.layers.0.input_layernorm', 'model.layers.0.post_attention_layernorm', 'model.layers.1', 'model.layers.1.self_attn', 'model.layers.1.self_attn.q_proj', 'model.layers.1.self_attn.k_proj', 'model.layers.1.self_attn.v_proj', 'model.layers.1.self_attn.o_proj', 'model.layers.1.self_attn.rotary_emb', 'model.layers.1.mlp', 'model.layers.1.mlp.gate_proj', 'model.layers.1.mlp.down_proj', 'model.layers.1.mlp.up_proj', 'model.layers.1.mlp.act_fn', 'model.layers.1.input_layernorm', 'model.layers.1.post_attention_layernorm', 'model.layers.2', 'model.layers.2.self_attn', 'model.layers.2.self_attn.q_proj', 'model.layers.2.self_attn.k_proj', 'model.layers.2.self_attn.v_proj', 'model.layers.2.self_attn.o_proj', 'model.layers.2.self_attn.rotary_emb', 'model.layers.2.mlp', 'model.layers.2.mlp.gate_proj', 'model.layers.2.mlp.down_proj', 'model.layers.2.mlp.up_proj', 'model.layers.2.mlp.act_fn', 'model.layers.2.input_layernorm', 'model.layers.2.post_attention_layernorm', 'model.layers.3', 'model.layers.3.self_attn', 'model.layers.3.self_attn.q_proj', 'model.layers.3.self_attn.k_proj', 'model.layers.3.self_attn.v_proj', 'model.layers.3.self_attn.o_proj', 'model.layers.3.self_attn.rotary_emb', 'model.layers.3.mlp', 'model.layers.3.mlp.gate_proj', 'model.layers.3.mlp.down_proj', 'model.layers.3.mlp.up_proj', 'model.layers.3.mlp.act_fn', 'model.layers.3.input_layernorm', 'model.layers.3.post_attention_layernorm', 'model.layers.4', 'model.layers.4.self_attn', 'model.layers.4.self_attn.q_proj', 'model.layers.4.self_attn.k_proj', 'model.layers.4.self_attn.v_proj', 'model.layers.4.self_attn.o_proj', 'model.layers.4.self_attn.rotary_emb', 'model.layers.4.mlp', 'model.layers.4.mlp.gate_proj', 'model.layers.4.mlp.down_proj', 'model.layers.4.mlp.up_proj', 'model.layers.4.mlp.act_fn', 'model.layers.4.input_layernorm', 'model.layers.4.post_attention_layernorm', 'model.layers.5', 'model.layers.5.self_attn', 'model.layers.5.self_attn.q_proj', 'model.layers.5.self_attn.k_proj', 'model.layers.5.self_attn.v_proj', 'model.layers.5.self_attn.o_proj', 'model.layers.5.self_attn.rotary_emb', 'model.layers.5.mlp', 'model.layers.5.mlp.gate_proj', 'model.layers.5.mlp.down_proj', 'model.layers.5.mlp.up_proj', 'model.layers.5.mlp.act_fn', 'model.layers.5.input_layernorm', 'model.layers.5.post_attention_layernorm', 'model.layers.6', 'model.layers.6.self_attn', 'model.layers.6.self_attn.q_proj', 'model.layers.6.self_attn.k_proj', 'model.layers.6.self_attn.v_proj', 'model.layers.6.self_attn.o_proj', 'model.layers.6.self_attn.rotary_emb', 'model.layers.6.mlp', 'model.layers.6.mlp.gate_proj', 'model.layers.6.mlp.down_proj', 'model.layers.6.mlp.up_proj', 'model.layers.6.mlp.act_fn', 'model.layers.6.input_layernorm', 'model.layers.6.post_attention_layernorm', 'model.layers.7', 'model.layers.7.self_attn', 'model.layers.7.self_attn.q_proj', 'model.layers.7.self_attn.k_proj', 'model.layers.7.self_attn.v_proj', 'model.layers.7.self_attn.o_proj', 'model.layers.7.self_attn.rotary_emb', 'model.layers.7.mlp', 'model.layers.7.mlp.gate_proj', 'model.layers.7.mlp.down_proj', 'model.layers.7.mlp.up_proj', 'model.layers.7.mlp.act_fn', 'model.layers.7.input_layernorm', 'model.layers.7.post_attention_layernorm', 'model.layers.8', 'model.layers.8.self_attn', 'model.layers.8.self_attn.q_proj', 'model.layers.8.self_attn.k_proj', 'model.layers.8.self_attn.v_proj', 'model.layers.8.self_attn.o_proj', 'model.layers.8.self_attn.rotary_emb', 'model.layers.8.mlp', 'model.layers.8.mlp.gate_proj', 'model.layers.8.mlp.down_proj', 'model.layers.8.mlp.up_proj', 'model.layers.8.mlp.act_fn', 'model.layers.8.input_layernorm', 'model.layers.8.post_attention_layernorm', 'model.layers.9', 'model.layers.9.self_attn', 'model.layers.9.self_attn.q_proj', 'model.layers.9.self_attn.k_proj', 'model.layers.9.self_attn.v_proj', 'model.layers.9.self_attn.o_proj', 'model.layers.9.self_attn.rotary_emb', 'model.layers.9.mlp', 'model.layers.9.mlp.gate_proj', 'model.layers.9.mlp.down_proj', 'model.layers.9.mlp.up_proj', 'model.layers.9.mlp.act_fn', 'model.layers.9.input_layernorm', 'model.layers.9.post_attention_layernorm', 'model.layers.10', 'model.layers.10.self_attn', 'model.layers.10.self_attn.q_proj', 'model.layers.10.self_attn.k_proj', 'model.layers.10.self_attn.v_proj', 'model.layers.10.self_attn.o_proj', 'model.layers.10.self_attn.rotary_emb', 'model.layers.10.mlp', 'model.layers.10.mlp.gate_proj', 'model.layers.10.mlp.down_proj', 'model.layers.10.mlp.up_proj', 'model.layers.10.mlp.act_fn', 'model.layers.10.input_layernorm', 'model.layers.10.post_attention_layernorm', 'model.layers.11', 'model.layers.11.self_attn', 'model.layers.11.self_attn.q_proj', 'model.layers.11.self_attn.k_proj', 'model.layers.11.self_attn.v_proj', 'model.layers.11.self_attn.o_proj', 'model.layers.11.self_attn.rotary_emb', 'model.layers.11.mlp', 'model.layers.11.mlp.gate_proj', 'model.layers.11.mlp.down_proj', 'model.layers.11.mlp.up_proj', 'model.layers.11.mlp.act_fn', 'model.layers.11.input_layernorm', 'model.layers.11.post_attention_layernorm', 'model.layers.12', 'model.layers.12.self_attn', 'model.layers.12.self_attn.q_proj', 'model.layers.12.self_attn.k_proj', 'model.layers.12.self_attn.v_proj', 'model.layers.12.self_attn.o_proj', 'model.layers.12.self_attn.rotary_emb', 'model.layers.12.mlp', 'model.layers.12.mlp.gate_proj', 'model.layers.12.mlp.down_proj', 'model.layers.12.mlp.up_proj', 'model.layers.12.mlp.act_fn', 'model.layers.12.input_layernorm', 'model.layers.12.post_attention_layernorm', 'model.layers.13', 'model.layers.13.self_attn', 'model.layers.13.self_attn.q_proj', 'model.layers.13.self_attn.k_proj', 'model.layers.13.self_attn.v_proj', 'model.layers.13.self_attn.o_proj', 'model.layers.13.self_attn.rotary_emb', 'model.layers.13.mlp', 'model.layers.13.mlp.gate_proj', 'model.layers.13.mlp.down_proj', 'model.layers.13.mlp.up_proj', 'model.layers.13.mlp.act_fn', 'model.layers.13.input_layernorm', 'model.layers.13.post_attention_layernorm', 'model.layers.14', 'model.layers.14.self_attn', 'model.layers.14.self_attn.q_proj', 'model.layers.14.self_attn.k_proj', 'model.layers.14.self_attn.v_proj', 'model.layers.14.self_attn.o_proj', 'model.layers.14.self_attn.rotary_emb', 'model.layers.14.mlp', 'model.layers.14.mlp.gate_proj', 'model.layers.14.mlp.down_proj', 'model.layers.14.mlp.up_proj', 'model.layers.14.mlp.act_fn', 'model.layers.14.input_layernorm', 'model.layers.14.post_attention_layernorm', 'model.layers.15', 'model.layers.15.self_attn', 'model.layers.15.self_attn.q_proj', 'model.layers.15.self_attn.k_proj', 'model.layers.15.self_attn.v_proj', 'model.layers.15.self_attn.o_proj', 'model.layers.15.self_attn.rotary_emb', 'model.layers.15.mlp', 'model.layers.15.mlp.gate_proj', 'model.layers.15.mlp.down_proj', 'model.layers.15.mlp.up_proj', 'model.layers.15.mlp.act_fn', 'model.layers.15.input_layernorm', 'model.layers.15.post_attention_layernorm', 'model.layers.16', 'model.layers.16.self_attn', 'model.layers.16.self_attn.q_proj', 'model.layers.16.self_attn.k_proj', 'model.layers.16.self_attn.v_proj', 'model.layers.16.self_attn.o_proj', 'model.layers.16.self_attn.rotary_emb', 'model.layers.16.mlp', 'model.layers.16.mlp.gate_proj', 'model.layers.16.mlp.down_proj', 'model.layers.16.mlp.up_proj', 'model.layers.16.mlp.act_fn', 'model.layers.16.input_layernorm', 'model.layers.16.post_attention_layernorm', 'model.layers.17', 'model.layers.17.self_attn', 'model.layers.17.self_attn.q_proj', 'model.layers.17.self_attn.k_proj', 'model.layers.17.self_attn.v_proj', 'model.layers.17.self_attn.o_proj', 'model.layers.17.self_attn.rotary_emb', 'model.layers.17.mlp', 'model.layers.17.mlp.gate_proj', 'model.layers.17.mlp.down_proj', 'model.layers.17.mlp.up_proj', 'model.layers.17.mlp.act_fn', 'model.layers.17.input_layernorm', 'model.layers.17.post_attention_layernorm', 'model.layers.18', 'model.layers.18.self_attn', 'model.layers.18.self_attn.q_proj', 'model.layers.18.self_attn.k_proj', 'model.layers.18.self_attn.v_proj', 'model.layers.18.self_attn.o_proj', 'model.layers.18.self_attn.rotary_emb', 'model.layers.18.mlp', 'model.layers.18.mlp.gate_proj', 'model.layers.18.mlp.down_proj', 'model.layers.18.mlp.up_proj', 'model.layers.18.mlp.act_fn', 'model.layers.18.input_layernorm', 'model.layers.18.post_attention_layernorm', 'model.layers.19', 'model.layers.19.self_attn', 'model.layers.19.self_attn.q_proj', 'model.layers.19.self_attn.k_proj', 'model.layers.19.self_attn.v_proj', 'model.layers.19.self_attn.o_proj', 'model.layers.19.self_attn.rotary_emb', 'model.layers.19.mlp', 'model.layers.19.mlp.gate_proj', 'model.layers.19.mlp.down_proj', 'model.layers.19.mlp.up_proj', 'model.layers.19.mlp.act_fn', 'model.layers.19.input_layernorm', 'model.layers.19.post_attention_layernorm', 'model.layers.20', 'model.layers.20.self_attn', 'model.layers.20.self_attn.q_proj', 'model.layers.20.self_attn.k_proj', 'model.layers.20.self_attn.v_proj', 'model.layers.20.self_attn.o_proj', 'model.layers.20.self_attn.rotary_emb', 'model.layers.20.mlp', 'model.layers.20.mlp.gate_proj', 'model.layers.20.mlp.down_proj', 'model.layers.20.mlp.up_proj', 'model.layers.20.mlp.act_fn', 'model.layers.20.input_layernorm', 'model.layers.20.post_attention_layernorm', 'model.layers.21', 'model.layers.21.self_attn', 'model.layers.21.self_attn.q_proj', 'model.layers.21.self_attn.k_proj', 'model.layers.21.self_attn.v_proj', 'model.layers.21.self_attn.o_proj', 'model.layers.21.self_attn.rotary_emb', 'model.layers.21.mlp', 'model.layers.21.mlp.gate_proj', 'model.layers.21.mlp.down_proj', 'model.layers.21.mlp.up_proj', 'model.layers.21.mlp.act_fn', 'model.layers.21.input_layernorm', 'model.layers.21.post_attention_layernorm', 'model.layers.22', 'model.layers.22.self_attn', 'model.layers.22.self_attn.q_proj', 'model.layers.22.self_attn.k_proj', 'model.layers.22.self_attn.v_proj', 'model.layers.22.self_attn.o_proj', 'model.layers.22.self_attn.rotary_emb', 'model.layers.22.mlp', 'model.layers.22.mlp.gate_proj', 'model.layers.22.mlp.down_proj', 'model.layers.22.mlp.up_proj', 'model.layers.22.mlp.act_fn', 'model.layers.22.input_layernorm', 'model.layers.22.post_attention_layernorm', 'model.layers.23', 'model.layers.23.self_attn', 'model.layers.23.self_attn.q_proj', 'model.layers.23.self_attn.k_proj', 'model.layers.23.self_attn.v_proj', 'model.layers.23.self_attn.o_proj', 'model.layers.23.self_attn.rotary_emb', 'model.layers.23.mlp', 'model.layers.23.mlp.gate_proj', 'model.layers.23.mlp.down_proj', 'model.layers.23.mlp.up_proj', 'model.layers.23.mlp.act_fn', 'model.layers.23.input_layernorm', 'model.layers.23.post_attention_layernorm', 'model.layers.24', 'model.layers.24.self_attn', 'model.layers.24.self_attn.q_proj', 'model.layers.24.self_attn.k_proj', 'model.layers.24.self_attn.v_proj', 'model.layers.24.self_attn.o_proj', 'model.layers.24.self_attn.rotary_emb', 'model.layers.24.mlp', 'model.layers.24.mlp.gate_proj', 'model.layers.24.mlp.down_proj', 'model.layers.24.mlp.up_proj', 'model.layers.24.mlp.act_fn', 'model.layers.24.input_layernorm', 'model.layers.24.post_attention_layernorm', 'model.layers.25', 'model.layers.25.self_attn', 'model.layers.25.self_attn.q_proj', 'model.layers.25.self_attn.k_proj', 'model.layers.25.self_attn.v_proj', 'model.layers.25.self_attn.o_proj', 'model.layers.25.self_attn.rotary_emb', 'model.layers.25.mlp', 'model.layers.25.mlp.gate_proj', 'model.layers.25.mlp.down_proj', 'model.layers.25.mlp.up_proj', 'model.layers.25.mlp.act_fn', 'model.layers.25.input_layernorm', 'model.layers.25.post_attention_layernorm', 'model.layers.26', 'model.layers.26.self_attn', 'model.layers.26.self_attn.q_proj', 'model.layers.26.self_attn.k_proj', 'model.layers.26.self_attn.v_proj', 'model.layers.26.self_attn.o_proj', 'model.layers.26.self_attn.rotary_emb', 'model.layers.26.mlp', 'model.layers.26.mlp.gate_proj', 'model.layers.26.mlp.down_proj', 'model.layers.26.mlp.up_proj', 'model.layers.26.mlp.act_fn', 'model.layers.26.input_layernorm', 'model.layers.26.post_attention_layernorm', 'model.layers.27', 'model.layers.27.self_attn', 'model.layers.27.self_attn.q_proj', 'model.layers.27.self_attn.k_proj', 'model.layers.27.self_attn.v_proj', 'model.layers.27.self_attn.o_proj', 'model.layers.27.self_attn.rotary_emb', 'model.layers.27.mlp', 'model.layers.27.mlp.gate_proj', 'model.layers.27.mlp.down_proj', 'model.layers.27.mlp.up_proj', 'model.layers.27.mlp.act_fn', 'model.layers.27.input_layernorm', 'model.layers.27.post_attention_layernorm', 'model.layers.28', 'model.layers.28.self_attn', 'model.layers.28.self_attn.q_proj', 'model.layers.28.self_attn.k_proj', 'model.layers.28.self_attn.v_proj', 'model.layers.28.self_attn.o_proj', 'model.layers.28.self_attn.rotary_emb', 'model.layers.28.mlp', 'model.layers.28.mlp.gate_proj', 'model.layers.28.mlp.down_proj', 'model.layers.28.mlp.up_proj', 'model.layers.28.mlp.act_fn', 'model.layers.28.input_layernorm', 'model.layers.28.post_attention_layernorm', 'model.layers.29', 'model.layers.29.self_attn', 'model.layers.29.self_attn.q_proj', 'model.layers.29.self_attn.k_proj', 'model.layers.29.self_attn.v_proj', 'model.layers.29.self_attn.o_proj', 'model.layers.29.self_attn.rotary_emb', 'model.layers.29.mlp', 'model.layers.29.mlp.gate_proj', 'model.layers.29.mlp.down_proj', 'model.layers.29.mlp.up_proj', 'model.layers.29.mlp.act_fn', 'model.layers.29.input_layernorm', 'model.layers.29.post_attention_layernorm', 'model.layers.30', 'model.layers.30.self_attn', 'model.layers.30.self_attn.q_proj', 'model.layers.30.self_attn.k_proj', 'model.layers.30.self_attn.v_proj', 'model.layers.30.self_attn.o_proj', 'model.layers.30.self_attn.rotary_emb', 'model.layers.30.mlp', 'model.layers.30.mlp.gate_proj', 'model.layers.30.mlp.down_proj', 'model.layers.30.mlp.up_proj', 'model.layers.30.mlp.act_fn', 'model.layers.30.input_layernorm', 'model.layers.30.post_attention_layernorm', 'model.layers.31', 'model.layers.31.self_attn', 'model.layers.31.self_attn.q_proj', 'model.layers.31.self_attn.k_proj', 'model.layers.31.self_attn.v_proj', 'model.layers.31.self_attn.o_proj', 'model.layers.31.self_attn.rotary_emb', 'model.layers.31.mlp', 'model.layers.31.mlp.gate_proj', 'model.layers.31.mlp.down_proj', 'model.layers.31.mlp.up_proj', 'model.layers.31.mlp.act_fn', 'model.layers.31.input_layernorm', 'model.layers.31.post_attention_layernorm', 'model.norm', 'lm_head']\n",
      "Target modules: ['query_key_value']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules ['query_key_value'] not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m     22\u001b[0m config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     23\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[1;32m     24\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m print_trainable_parameters(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/peft/mapping.py:122\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m    121\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/peft/peft_model.py:798\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/peft/peft_model.py:106\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m peft_config\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mPEFT_TYPE_TO_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/peft/tuners/lora.py:154\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/peft/tuners/lora.py:162\u001b[0m, in \u001b[0;36mLoraModel.add_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(adapter_name)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for all adapters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/peft/tuners/lora.py:257\u001b[0m, in \u001b[0;36mLoraModel._find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_module(parent, target_name, new_module, target)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules ['query_key_value'] not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch200]",
   "language": "python",
   "name": "conda-env-torch200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
