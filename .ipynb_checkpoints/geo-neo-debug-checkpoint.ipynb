{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f691eb20-99ad-44c2-a384-0376451cd99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import math\n",
    "\n",
    "SEQ_LEN = 1000\n",
    "RANGE_START = -1\n",
    "RANGE_END = 1\n",
    "EMBED_SIZE = 768\n",
    "LOWER_BOUND = -10\n",
    "UPPER_BOUND = 10\n",
    "\n",
    "# Input:\n",
    "#    formula: A formula containing \"x\", which will be replaced to numbers between range_start and range_end\n",
    "# Output:\n",
    "#    a sequence of embeddings, each has embed_size dimensions, and each dimension is between LOWER_BOUND and UPPER_BOUND\n",
    "def generate_seq_embed(func,\n",
    "                       seq_len = SEQ_LEN,\n",
    "                       range_start = RANGE_START,\n",
    "                       range_end = RANGE_END,\n",
    "                       embed_size = EMBED_SIZE):\n",
    "    start = time.time()\n",
    "    seq = torch.zeros(seq_len, embed_size)\n",
    "    step = (range_end - range_start) / (seq_len*embed_size - 1)\n",
    "    for i in range(seq_len*embed_size):\n",
    "        x = range_start + i * step\n",
    "        #print(formula.replace('x', str(x)))\n",
    "        #y = eval(formula.replace('x', str(x)))\n",
    "        y = func(x)\n",
    "        y = max(LOWER_BOUND, min(UPPER_BOUND,y))\n",
    "        seq[i // embed_size][i % embed_size] = y\n",
    "    end = time.time()\n",
    "    print(end - start, \"seconds\")\n",
    "    return seq\n",
    "\n",
    "#seq = generate_seq_embed(eval(\"lambda x: math.exp(-0.5*((x-1)**2))\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0330a445-f7e0-4080-88ea-6fcb7055c9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/home/xiaoxin_yin/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q_id': 'km8e9',\n",
       " 'title': 'Can someone explain how the following two phenomenon happen scientifically?',\n",
       " 'selftext': 'In an argument about creation vs evolution, my creationist uncle asked me to explain the following two questions.  If anyone can provide answers or links to answers, I\\'d surely be grateful:\\n\\n\"What about the fact that stinking dong at the foot of a rose is turned by that soil into fragrance in the rose flower?\"\\n\\n\"Or two atoms of hydrogen and one atom of oxygen, two odorless and tasteless gases, combine to produce water, without which life on earth would just be impossible.\"\\n\\nHe then says something else and asks:\\n\\n\"But can science, again the basis of explanation of evolution, account for how they (the two above questions) are done?\"\\n\\nThanks guys!  Also, I should mention that he was quite offended when I retorted with a \"hee hee\" at \"stinking dong\" of a flower.  :)\\n\\n**edit:  I think by \"dong\" he may have meant \"dung\", but I\\'m not sure.  English is not his primary language.**',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['c2lebcr', 'c2lebu9'],\n",
       " 'answers.text': ['The problem with answering these questions is that they are \"loaded\".  He is making an aesthetic argument rather than a scientific one.\\n\\nHow a plant utilizes fertilizer, how hydrogen bonds to oxygen to form water, etc is very thoroughly documented. What he is asking is \"why is it that a rose will turn dung into something beautiful?\", and \"why is it that the universe would work out in such a way to allow water to form in the first place?\"\\n\\nNeither of the questions he is actually asking has a scientific answer because they are fundamentally philosophical. If you want to know exactly how the rose bush works, look [here](_URL_0_), or for how water bonds [here](_URL_1_). Neither of these will give him an answer that he finds satisfactory though.',\n",
       "  \"These questions have nothing to do with creationism or evolution.\\n\\nThe stinking dung is rich in nutrients that the plant needs.  It can metabolically break down those components (which are nitrogen rich), and then using biochemical processes, it can rebuild those components into what it needs.  It's simple chemistry.\\n\\nWith water, hydrogen and oxygen as gases are diatomic (meaning that the gases themselves are H2 and O2).  When combined, as H2O, they have different properties because they are chemically different.  The hydrogen and oxygen are now sharing electrons, and the molecule itself is polar, which allows for such different properties.\\n\\nRegardless, these have nothing to do with evolution.\"],\n",
       " 'answers.score': [3, 2],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': ['http://en.wikipedia.org/wiki/Fertilizer',\n",
       "  'http://en.wikipedia.org/wiki/Chemical_bond']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: https://huggingface.co/docs/transformers/v4.17.0/en/tasks/language_modeling\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")\n",
    "eli5 = eli5.train_test_split(test_size=0.1)\n",
    "\n",
    "#\n",
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81463581-4550-4e5b-abd3-e54627325a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': 'km8e9',\n",
       " 'title': 'Can someone explain how the following two phenomenon happen scientifically?',\n",
       " 'selftext': 'In an argument about creation vs evolution, my creationist uncle asked me to explain the following two questions.  If anyone can provide answers or links to answers, I\\'d surely be grateful:\\n\\n\"What about the fact that stinking dong at the foot of a rose is turned by that soil into fragrance in the rose flower?\"\\n\\n\"Or two atoms of hydrogen and one atom of oxygen, two odorless and tasteless gases, combine to produce water, without which life on earth would just be impossible.\"\\n\\nHe then says something else and asks:\\n\\n\"But can science, again the basis of explanation of evolution, account for how they (the two above questions) are done?\"\\n\\nThanks guys!  Also, I should mention that he was quite offended when I retorted with a \"hee hee\" at \"stinking dong\" of a flower.  :)\\n\\n**edit:  I think by \"dong\" he may have meant \"dung\", but I\\'m not sure.  English is not his primary language.**',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['c2lebcr', 'c2lebu9'],\n",
       " 'answers.text': ['The problem with answering these questions is that they are \"loaded\".  He is making an aesthetic argument rather than a scientific one.\\n\\nHow a plant utilizes fertilizer, how hydrogen bonds to oxygen to form water, etc is very thoroughly documented. What he is asking is \"why is it that a rose will turn dung into something beautiful?\", and \"why is it that the universe would work out in such a way to allow water to form in the first place?\"\\n\\nNeither of the questions he is actually asking has a scientific answer because they are fundamentally philosophical. If you want to know exactly how the rose bush works, look [here](_URL_0_), or for how water bonds [here](_URL_1_). Neither of these will give him an answer that he finds satisfactory though.',\n",
       "  \"These questions have nothing to do with creationism or evolution.\\n\\nThe stinking dung is rich in nutrients that the plant needs.  It can metabolically break down those components (which are nitrogen rich), and then using biochemical processes, it can rebuild those components into what it needs.  It's simple chemistry.\\n\\nWith water, hydrogen and oxygen as gases are diatomic (meaning that the gases themselves are H2 and O2).  When combined, as H2O, they have different properties because they are chemically different.  The hydrogen and oxygen are now sharing electrons, and the molecule itself is polar, which allows for such different properties.\\n\\nRegardless, these have nothing to do with evolution.\"],\n",
       " 'answers.score': [3, 2],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': ['http://en.wikipedia.org/wiki/Fertilizer',\n",
       "  'http://en.wikipedia.org/wiki/Chemical_bond']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f50c9a-3f89-4b28-b551-c86b900e57f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]], padding=True, truncation=True)\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb89255-9a0c-4856-a27f-d20542e7bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# new_column = np.ones((len(lm_dataset['train']), 768))\n",
    "# lm_dataset['train'] = lm_dataset['train'].add_column(\"inputs_embeds\", new_column.tolist())\n",
    "\n",
    "new_column = np.ones((len(lm_dataset['test']), 768))\n",
    "lm_dataset['test'] = lm_dataset['test'].add_column(\"inputs_embeds\", new_column.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3404d2-36ee-4065-a773-bb88219f0a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dataset['test'][123]['inputs_embeds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5004b9e3-5d1e-42fa-a5a1-79513d7c6b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 05:34:28.529282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c53b9af4-4192-4d55-ba88-f7db91a77d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyGPTNeoForCausalLM were not initialized from the model checkpoint at EleutherAI/gpt-neo-125m and are newly initialized: ['lm_head.weight', 'transformer.h.11.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.1.attn.attention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "from transformers.models.gpt_neo.configuration_gpt_neo import GPTNeoConfig\n",
    "from transformers.models.gpt_neo.modeling_gpt_neo import *\n",
    "\n",
    "class MyGPTNeoForCausalLM(GPTNeoPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPTNeoModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past_key_values:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"position_ids\": position_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "        #print(inputs_embeds)\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        inputs_embeds = self.transformer.wte(input_ids)\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            None, #input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            # Compute loss in fp32 to match with mesh-tf version\n",
    "            # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n",
    "            lm_logits = lm_logits.to(torch.float32)\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            lm_logits = lm_logits.to(hidden_states.dtype)\n",
    "            loss = loss.to(hidden_states.dtype)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(\n",
    "        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n",
    "    ) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n",
    "        [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n",
    "        beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past_key_values\n",
    "        )\n",
    "\n",
    "model = MyGPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b9b4e-da59-4b63-8ae6-227e14392a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='568' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 568/1500 02:01 < 03:20, 4.64 it/s, Epoch 1.13/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.185000</td>\n",
       "      <td>0.566312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_14253/3305847346.py\", line 21, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/trainer.py\", line 1649, in train\n",
      "    ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/trainer.py\", line 1916, in _inner_training_loop\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 530, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 570, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 45, in __call__\n",
      "    return self.torch_call(features)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 732, in torch_call\n",
      "    batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 3058, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 211, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 731, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_14253/3305847346.py\", line 21, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/trainer.py\", line 1649, in train\n",
      "    ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/trainer.py\", line 1916, in _inner_training_loop\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 530, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 570, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 45, in __call__\n",
      "    return self.torch_call(features)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 732, in torch_call\n",
      "    batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 3058, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 211, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 731, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/rich/traceback.py\", line 130, in ipy_show_traceback\n",
      "    default_showtraceback(*args, **kwargs)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/posixpath.py\", line 443, in _joinrealpath\n",
      "    path, ok = _joinrealpath(path, os.readlink(newpath), seen)\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/default_user/.conda/envs/user/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"test\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (User)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
