{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb -O /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb && \\\n",
    "sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-3_11.5.1.109-1_amd64.deb -O /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb && \\\n",
    "sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb -O /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb && \\\n",
    "sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-3_10.2.4.109-1_amd64.deb -O /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb && \\\n",
    "sudo dpkg -i /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb && \\\n",
    "sudo dpkg -i /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb && \\\n",
    "sudo dpkg -i /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb && \\\n",
    "sudo dpkg -i /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate<1,>=0.16.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.21.0.dev0)\n",
      "Requirement already satisfied: click<9,>=8.0.4 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (8.1.3)\n",
      "Requirement already satisfied: datasets<3,>=2.10.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.13.0)\n",
      "Requirement already satisfied: deepspeed<0.9,>=0.8.3 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.8.3)\n",
      "Requirement already satisfied: transformers[torch]<5,>=4.28.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.30.2)\n",
      "Requirement already satisfied: langchain>=0.0.139 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.0.205)\n",
      "Requirement already satisfied: torch<2,>=1.13.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from accelerate<1,>=0.16.0->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from accelerate<1,>=0.16.0->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: psutil in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from accelerate<1,>=0.16.0->-r requirements.txt (line 1)) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from accelerate<1,>=0.16.0->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from datasets<3,>=2.10.0->-r requirements.txt (line 3)) (0.15.1)\n",
      "Requirement already satisfied: hjson in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from deepspeed<0.9,>=0.8.3->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: ninja in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from deepspeed<0.9,>=0.8.3->-r requirements.txt (line 4)) (1.11.1)\n",
      "Requirement already satisfied: py-cpuinfo in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from deepspeed<0.9,>=0.8.3->-r requirements.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from deepspeed<0.9,>=0.8.3->-r requirements.txt (line 4)) (1.10.9)\n",
      "Requirement already satisfied: filelock in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from transformers[torch]<5,>=4.28.1->-r requirements.txt (line 5)) (3.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from transformers[torch]<5,>=4.28.1->-r requirements.txt (line 5)) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from transformers[torch]<5,>=4.28.1->-r requirements.txt (line 5)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from transformers[torch]<5,>=4.28.1->-r requirements.txt (line 5)) (0.3.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from langchain>=0.0.139->-r requirements.txt (line 6)) (2.0.16)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from langchain>=0.0.139->-r requirements.txt (line 6)) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from langchain>=0.0.139->-r requirements.txt (line 6)) (0.5.8)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.9 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from langchain>=0.0.139->-r requirements.txt (line 6)) (0.0.13)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from langchain>=0.0.139->-r requirements.txt (line 6)) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from langchain>=0.0.139->-r requirements.txt (line 6)) (1.2.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from langchain>=0.0.139->-r requirements.txt (line 6)) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from torch<2,>=1.13.1->-r requirements.txt (line 7)) (4.6.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from torch<2,>=1.13.1->-r requirements.txt (line 7)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from torch<2,>=1.13.1->-r requirements.txt (line 7)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from torch<2,>=1.13.1->-r requirements.txt (line 7)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from torch<2,>=1.13.1->-r requirements.txt (line 7)) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.13.1->-r requirements.txt (line 7)) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.13.1->-r requirements.txt (line 7)) (0.38.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from aiohttp->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.139->-r requirements.txt (line 6)) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.139->-r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.139->-r requirements.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.139->-r requirements.txt (line 6)) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from pandas->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from pandas->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from pandas->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets<3,>=2.10.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.139->-r requirements.txt (line 6)) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< [ { ( 4 * 8 = 32 ) shifted 3 = 32000 } + { ( 9 * 8 = 72 ) shifted 2 = 7200 } + { ( 9 * 8 = 72 ) shifted 1 = 720 } + ( 9 * 8 = 72 ) = 39992 ] shifted 1 = 399920 > + [ { ( 4 * 1 = 4 ) shifted 3 = 4000 } + { ( 9 * 1 = 9 ) shifted 2 = 900 } + { ( 9 * 1 = 9 ) shifted 1 = 90 } + ( 9 * 1 = 9 ) = 4999 ] = 3 9 9 9 2 0 + 4 9 9 9 = 4 0 4 9 1 9 '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def space(a):\n",
    "    res=\"\"\n",
    "    for i in range(len(a)):\n",
    "        res+=a[i]\n",
    "        if i!=len(a)-1:\n",
    "            res+=\" \"\n",
    "    return res\n",
    "def to_words_mult(a,b):\n",
    "    res=\"\"\n",
    "    for i in range(len(b)):\n",
    "        if i!=len(b)-1:\n",
    "            res+=\"< \"\n",
    "        res+=\"[ \"\n",
    "        for j in range(len(a)):\n",
    "            if j!=len(a)-1:\n",
    "                res+=\"{ \"\n",
    "            if len(a)-j>1:\n",
    "                res+=\"( \"+a[j]+\" * \"+b[i]+\" = \"+str(int(a[j])*int(b[i]))+\" ) \"\n",
    "                res+=\"shifted \"+str(len(a)-j-1)\n",
    "                res+=\" = \"+str(int(a[j])*int(b[i])*10**(len(a)-j-1))\n",
    "            else:\n",
    "                res+=\"( \"+a[j]+\" * \"+b[i]+\" = \"+str(int(a[j])*int(b[i]))+\" )\"\n",
    "            if j!=len(a)-1:\n",
    "                res+=\" }\"\n",
    "            if j!=len(a)-1:\n",
    "                res+=\" + \"\n",
    "        res+=\" = \"+str(int(a)*int(b[i]))\n",
    "        res+=\" ] \"\n",
    "        if len(b)-i>1:\n",
    "            res+=\"shifted \"+str(len(b)-i-1) \n",
    "            res+=\" = \"+str(int(a)*int(b[i])*10**(len(b)-i-1))\n",
    "            res+=\" \"\n",
    "        if i!=len(b)-1:\n",
    "            res+=\">\"\n",
    "        if i!=len(b)-1:\n",
    "            res+=\" + \"\n",
    "    res+=\"= \"\n",
    "    nums=[]\n",
    "    for i in range(len(b)):\n",
    "        num=str(int(a)*int(b[i])*10**(len(b)-i-1))\n",
    "        for j in str(int(a)*int(b[i])*10**(len(b)-i-1)):\n",
    "            res+=j+\" \"\n",
    "        nums.append(num)\n",
    "        if i!=len(b)-1:\n",
    "            res+=\"+ \"\n",
    "    sum=int(nums[0])\n",
    "    for i in range(1,len(nums)):\n",
    "        sum+=int(nums[i])\n",
    "        if i==len(nums)-1:\n",
    "            res+=\"= \"\n",
    "            for j in str(int(a)*int(b)):\n",
    "                res+=j+\" \"\n",
    "            break\n",
    "        res+=\"= \"\n",
    "        for j in str(sum):\n",
    "            res+=j+\" \"\n",
    "        if i<len(nums)-1:\n",
    "            res+=\"+ \"\n",
    "            for j in range(i+1,len(nums)):\n",
    "                for k in nums[j]:\n",
    "                    res+=k+\" \"\n",
    "                if j!=len(nums)-1:\n",
    "                    res+=\"+\"\n",
    "    return res\n",
    "def to_words_div(a,b):\n",
    "    res=\"\"\n",
    "    res+=a+\" / \"+b+\" = \"+space(a)+\" / \"+space(b)\n",
    "    res+=\" =\"\n",
    "    rem=int(a)\n",
    "    res+=\" < \"\n",
    "    res+=\"[\"\n",
    "    mid_reses=[]\n",
    "    first=True\n",
    "    prev_remainder=-1\n",
    "    while rem>=int(b):\n",
    "        digits=1\n",
    "        while(int(str(rem)[:digits])<int(b)):\n",
    "            digits+=1\n",
    "        mid_res=math.floor(int(str(rem)[:digits])/int(b))\n",
    "        res+=\" { \"\n",
    "        res+=\"( \"\n",
    "        tmp=prev_remainder\n",
    "        prev_remainder=int(str(rem)[:digits])-mid_res*int(b)\n",
    "        if not first:\n",
    "            res+=space(str(tmp))+\" shifted 1\"+\" + \"+str(rem)[digits-1]+\" = \"\n",
    "        res+=space(str(rem)[:digits])+\" / \"+space(b)+\" #\"\n",
    "        res+=\" because \"+str(mid_res)+\" * \"+space(b)+\" <= \"+space(str(rem)[:digits])\n",
    "        res+=\" |\"\n",
    "        res+=\" \"+str(mid_res)+\",\"\n",
    "        res+=\" remainder \"+\"= \"+space(str(rem)[:digits])+\" - \"+str(mid_res)+\" * \"+space(b)+\" = \"+space(str(rem)[:digits])+\" - \"+space(str(mid_res*int(b)))+\" = \"+space(str(prev_remainder))+\" )\"\n",
    "        if len(str(rem))-digits>0:\n",
    "            res+=\" shifted \"+str(len(str(rem))-digits)\n",
    "        res+=\" = \"+space(str(mid_res*10**(len(str(rem))-digits)))\n",
    "        mid_reses.append(mid_res*(10**(len(str(rem))-digits)))\n",
    "        rem-=int(b)*mid_res*(10**(len(str(rem))-digits))\n",
    "        res+=\" }\"\n",
    "        if rem>=int(b):\n",
    "            res+=\" +\"  \n",
    "        first=False\n",
    "    res+=\" ]\"\n",
    "    sum=0\n",
    "    for i in range(len(mid_reses)):\n",
    "        sum+=int(mid_reses[i])\n",
    "        res+=\" = \"\n",
    "        res+=space(str(sum))\n",
    "        if i!=len(mid_reses)-1:\n",
    "            res+=\" + \"\n",
    "        for j in range(i+1,len(mid_reses)):\n",
    "            res+=space(str(mid_reses[j]))\n",
    "            if j!=len(mid_reses)-1:\n",
    "                res+=\" + \"\n",
    "    res+=\" >\"\n",
    "    res+=\" = \"+space(str(int(int(a)/int(b))))+\" remainder \"+space(str(rem))\n",
    "    return res\n",
    "def to_words_add(a,b):\n",
    "    res=\"\"\n",
    "    res+=a+\" + \"+b+\" = \"\n",
    "    a_spaced=space(a)\n",
    "    b_spaced=space(b)\n",
    "    res+=a_spaced+\" + \"+b_spaced+\" = \"\n",
    "    res+=space(str(int(a)+int(b)))\n",
    "    return res\n",
    "def to_words_sub(a,b):\n",
    "    res=\"\"\n",
    "    res+=a+\" - \"+b+\" = \"\n",
    "    a_spaced=space(a)    \n",
    "    b_spaced=space(b)\n",
    "    res+=a_spaced+\" - \"+b_spaced+\" = \"\n",
    "    res+=space(str(int(a)-int(b)))\n",
    "    return res\n",
    "to_words_mult(\"4999\",\"81\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "with open(\"/home/mcwave/data/multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "    outfile.write(\"\\n\")\n",
    "for i in range(120000):\n",
    "    operation=random.randint(1,6)\n",
    "    if str(operation) in \"12\":\n",
    "        a=random.randrange(2,9999)\n",
    "        b=random.randrange(2,9999)\n",
    "        instructions=str(a)+\" * \"+str(b)\n",
    "        response=\"\"\n",
    "        for char in str(a):\n",
    "            response+=char+\" \"\n",
    "        response+=\"* \"\n",
    "        for char in str(b):\n",
    "            response+=char+\" \"\n",
    "        response+=\"= \"+to_words_mult(str(a),str(b))\n",
    "        question={\"instruction\":instructions, \"context\": \"\", \"category\": \"open_qa\", \"response\": response}\n",
    "        with open(\"/home/mcwave/data/multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "            json.dump(question, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "    if str(operation) in \"34\":\n",
    "        digits_1=random.randint(2,4)\n",
    "        digits_2=random.randint(1,digits_1)\n",
    "        a=random.randint(10**(digits_1-1),(10**digits_1)-1)\n",
    "        b=random.randint(max(10**(digits_2-1),1),min((10**digits_2)-1,a))\n",
    "        instructions=str(a)+\" / \"+str(b)\n",
    "        response=\"\"\n",
    "        response+=to_words_div(str(a),str(b))\n",
    "        question={\"instruction\":instructions, \"context\": \"\", \"category\": \"open_qa\", \"response\": response}\n",
    "        with open(\"/home/mcwave/data/multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "            json.dump(question, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "    if operation==5:\n",
    "        a=random.randrange(2,9999)\n",
    "        b=random.randrange(2,9999)\n",
    "        instructions=str(a)+\" + \"+str(b)\n",
    "        response=\"\"\n",
    "        for char in str(a):\n",
    "            response+=char+\" \"\n",
    "        response+=\"+ \"\n",
    "        for char in str(b):\n",
    "            response+=char+\" \"\n",
    "        response+=\"= \"+to_words_add(str(a),str(b))\n",
    "        question={\"instruction\":instructions, \"context\": \"\", \"category\": \"open_qa\", \"response\": response}\n",
    "        with open(\"/home/mcwave/data/multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "            json.dump(question, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "    if operation==6:\n",
    "        a=random.randrange(3,9999)\n",
    "        b=random.randrange(2,a)\n",
    "        instructions=str(a)+\" - \"+str(b)\n",
    "        response=\"\"\n",
    "        for char in str(a):\n",
    "            response+=char+\" \"\n",
    "        response+=\"- \"\n",
    "        for char in str(b):\n",
    "            response+=char+\" \"\n",
    "        response+=\"= \"+to_words_sub(str(a),str(b))\n",
    "        question={\"instruction\":instructions, \"context\": \"\", \"category\": \"open_qa\", \"response\": response}\n",
    "        with open(\"/home/mcwave/data/multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "            json.dump(question, outfile)\n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "import os\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "timestamp = \"06292023\"\n",
    "model_name = \"dolly\"\n",
    "\n",
    "model_path = \"/home/mcwave/models/huggyllama-7b/llama-7b\"\n",
    "\n",
    "experiment_id = \"\"\n",
    "input_model = model_path\n",
    "\n",
    "if experiment_id:\n",
    "    experiment_id = re.sub(r\"\\s+\", \"_\", experiment_id.strip())\n",
    "    model_name = f\"{model_name}__{experiment_id}\"\n",
    "\n",
    "checkpoint_dir_name = \"model\"\n",
    "\n",
    "root_path = os.getcwd()\n",
    "deepspeed_config = os.path.join(root_path, \"config/dolly_config.json\")\n",
    "\n",
    "dolly_training_dir_name = \"dolly_training\"\n",
    "\n",
    "# Use the local training root path if it was provided.  Otherwise try to find a sensible default.\n",
    "local_training_root = model_path\n",
    "if not local_training_root:\n",
    "    # Use preferred path when working in a Databricks cluster if it exists.\n",
    "    if os.path.exists(\"/local_disk0\"):\n",
    "        local_training_root = os.path.join(\"/local_disk0\", dolly_training_dir_name)\n",
    "    # Otherwise use the home directory.\n",
    "    else:\n",
    "        local_training_root = os.path.join(os.path.expanduser('~'), dolly_training_dir_name)\n",
    "\n",
    "dbfs_output_root = \"/home/mcwave/code/results\"\n",
    "if not dbfs_output_root:\n",
    "    dbfs_output_root = f\"/dbfs/{dolly_training_dir_name}\"\n",
    "\n",
    "os.makedirs(local_training_root, exist_ok=True)\n",
    "os.makedirs(dbfs_output_root, exist_ok=True)\n",
    "\n",
    "local_output_dir = os.path.join(local_training_root, \"\")\n",
    "dbfs_output_dir = os.path.join(dbfs_output_root, checkpoint_dir_name)\n",
    "\n",
    "num_gpus_flag = \"\"\n",
    "num_gpus = \"2\"\n",
    "if num_gpus:\n",
    "    num_gpus = int(num_gpus)\n",
    "    num_gpus_flag = f\"--num_gpus={num_gpus}\"\n",
    "model_flag=f\"{model_path}\"\n",
    "\n",
    "tensorboard_display_dir = f\"{local_output_dir}/runs\"\n",
    "\n",
    "print(f\"Local Output Dir: {local_output_dir}\")\n",
    "print(f\"DBFS Output Dir: {dbfs_output_dir}\")\n",
    "print(f\"Tensorboard Display Dir: {tensorboard_display_dir}\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %load_ext tensorboard\n",
    "# MAGIC %tensorboard --logdir '{tensorboard_display_dir}'\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "!deepspeed {num_gpus_flag}\\\n",
    "     --module training.trainer \\\n",
    "     --input-model {input_model} \\\n",
    "     --deepspeed {deepspeed_config} \\\n",
    "     --epochs 3 \\\n",
    "     --local-output-dir {local_output_dir} \\\n",
    "     --dbfs-output-dir {dbfs_output_dir} \\\n",
    "     --per-device-train-batch-size 6 \\\n",
    "     --per-device-eval-batch-size 6 \\\n",
    "     --logging-steps 10 \\\n",
    "     --save-steps 500 \\\n",
    "     --save-total-limit 20 \\\n",
    "     --eval-steps 50 \\\n",
    "     --warmup-steps 50 \\\n",
    "     --test-size 200 \\\n",
    "     --lr 5e-6 \\\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# from training.generate import generate_response, load_model_tokenizer_for_generate\n",
    "\n",
    "# model, tokenizer = load_model_tokenizer_for_generate(input_model)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Examples from https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deepspeed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m     11\u001b[0m     AutoTokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     set_seed,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     DEFAULT_INPUT_MODEL,\n\u001b[1;32m     21\u001b[0m     DEFAULT_SEED,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     DEFAULT_TRAINING_DATASET,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     31\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mcwave/models/huggyllama-7b/llama-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m     33\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m     34\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m     bf16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m     37\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m---> 38\u001b[0m     deepspeed\u001b[38;5;241m=\u001b[39m\u001b[43mdeepspeed\u001b[49m,\n\u001b[1;32m     39\u001b[0m     gradient_checkpointing\u001b[38;5;241m=\u001b[39mgradient_checkpointing,\n\u001b[1;32m     40\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/runs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m     logging_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     42\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39mlogging_steps,\n\u001b[1;32m     43\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39meval_steps,\n\u001b[1;32m     45\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39msave_steps,\n\u001b[1;32m     47\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39msave_total_limit,\n\u001b[1;32m     48\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     50\u001b[0m     disable_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m     remove_unused_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m     local_rank\u001b[38;5;241m=\u001b[39mlocal_rank,\n\u001b[1;32m     53\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps,\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining args\u001b[39m\u001b[38;5;124m\"\u001b[39m,training_args)\n\u001b[1;32m     57\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstantiating Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deepspeed' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import click\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from training.consts import (\n",
    "    DEFAULT_INPUT_MODEL,\n",
    "    DEFAULT_SEED,\n",
    "    PROMPT_WITH_INPUT_FORMAT,\n",
    "    PROMPT_NO_INPUT_FORMAT,\n",
    "    END_KEY,\n",
    "    INSTRUCTION_KEY,\n",
    "    RESPONSE_KEY_NL,\n",
    "    DEFAULT_TRAINING_DATASET,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/mcwave/models/huggyllama-7b/llama-7b\",\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    deepspeed=deepspeed,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    logging_dir=f\"{local_output_dir}/runs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    disable_tqdm=True,\n",
    "    remove_unused_columns=False,\n",
    "    local_rank=local_rank,\n",
    "    warmup_steps=warmup_steps,\n",
    ")\n",
    "print(\"Training args\",training_args)\n",
    "\n",
    "logger.info(\"Instantiating Trainer\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections.abc import Mapping\n",
    "import torch\n",
    "isinstance(processed_dataset[0][\"input_ids\"],(tuple,list))\n",
    "#type(processed_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama-deepspeed/scripts/convert2hf.py\", line 90, in <module>\n",
      "    main()\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama-deepspeed/scripts/convert2hf.py\", line 82, in main\n",
      "    write_model(\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama-deepspeed/scripts/convert2hf.py\", line 46, in write_model\n",
      "    sd = torch.load(pt, map_location=\"cpu\")\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/serialization.py\", line 795, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/serialization.py\", line 1002, in _legacy_load\n",
      "    magic_number = pickle_module.load(f, **pickle_load_args)\n",
      "_pickle.UnpicklingError: pickle data was truncated\n",
      "cp: cannot stat '/home/mcwave/models/huggyllama-7b/llama-7b/checkpoint-1000/pytorch-model.bin': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python llama-deepspeed/scripts/convert2hf.py --model_size 7B \\\n",
    "    --input_dir /home/mcwave/models/huggyllama-7b/llama-7b/checkpoint-1000 \\\n",
    "    --output_dir /home/mcwave/models/huggyllama-7b/llama-7b/new-checkpoint-1000\n",
    "!cp /home/mcwave/models/huggyllama-7b/llama-7b/checkpoint-1000/pytorch-model.bin /home/mcwave/models/huggyllama-7b/llama-7b/new-model-1000\n",
    "!cp /home/mcwave/models/huggyllama-7b/llama-7b/checkpoint-1000/tokenizer.model /home/mcwave/models/huggyllama-7b/llama-7b/new-checkpoint-1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:/home/mcwave/models/huggyllama-7b/llama-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from training.generate import generate_response, load_model_tokenizer_for_generate\n",
    "\n",
    "model, tokenizer = load_model_tokenizer_for_generate(\"/home/mcwave/models/huggyllama-7b/llama-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/mcwave/.cache/huggingface/datasets/json/word_problems_train_data_2-774319a13887b267/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mcwave/code/ChatGLM-6B/ptuning/word_problems/word_problems_train_data_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1251.66it/s]\n",
      "Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems_train_data_2-774319a13887b267/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-cbd46533bca5e888.arrow\n",
      "Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems_train_data_2-774319a13887b267/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-66306f7c1dc2cbdd.arrow\n",
      "Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems_train_data_2-774319a13887b267/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-40b83aba8eba7c80.arrow\n",
      "Loading cached shuffled indices for dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems_train_data_2-774319a13887b267/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bae4d68796d4fa3b.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
       "          14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "          29889,    13,    13, 32001, 29871,    13, 18687, 13434, 29892,  5011,\n",
       "            265, 29892,   322,  5306,  1269, 18093,   263,   304, 29891, 22378,\n",
       "          29889,  5306, 29915, 29879, 22378,   338, 29871, 29945, 22831,  5520,\n",
       "           1135,  5011,   265, 29915, 29879, 22378, 29889,  5011,   265, 29915,\n",
       "          29879, 22378,   338, 29871, 29941, 22831,  5520,  1135,  8951,   278,\n",
       "           3309,   310, 18888, 29915, 29879, 22378, 29889, 18888, 29915, 29879,\n",
       "          22378,   338, 29871, 29896, 29945, 22831,  1472, 29889,  1128,  1784,\n",
       "          22831,  5520,   338,  5306, 29915, 29879, 22378,  1135, 18888, 29915,\n",
       "          29879, 22378, 29973,    13,    13, 32002,  8168,   625,   278,  3309,\n",
       "            310, 18888, 29915, 29879, 22378,   338, 29871, 29896, 29945,   921,\n",
       "          29871, 29906,   353,   518,   426,   313, 29871, 29896,   334, 29871,\n",
       "          29906,   353, 29871, 29906, 29889, 29900,  1723,  9500,   287, 29871,\n",
       "          29896,   353, 29871, 29906, 29900, 29900, 29900, 29900, 29900, 29900,\n",
       "          29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29889, 29900,\n",
       "            500,   718,   313, 29871, 29945,   334, 29871, 29906,   353, 29871,\n",
       "          29896, 29900, 29889, 29900,  1723,   353, 29871, 29941, 29900, 29889,\n",
       "          29900,  4514,   353, 29871, 29941, 29871, 29900,   869, 29871, 29900,\n",
       "          29871, 22831,    13,  6295, 29892,  5011,   265, 29915, 29879, 22378,\n",
       "            338, 29871, 29941, 29900,   718, 29871, 29941,   353,   529, 29871,\n",
       "          29941, 29871, 29900,   718, 29871, 29941,   353, 29871, 29941, 29871,\n",
       "          29941,   869, 29871, 29900,  1405, 22831,  1472,    13, 29967,  1540,\n",
       "          29915, 29879, 22378,   338, 29871, 29941, 29941,   718, 29871, 29945,\n",
       "            353,   529, 29871, 29941, 29871, 29941,   718, 29871, 29945,   353,\n",
       "          29871, 29941, 29871, 29947,   869, 29871, 29900,  1405, 22831,  1472,\n",
       "             13,  1349,   375, 29892,  5306, 29915, 29879, 22378,   338, 29871,\n",
       "          29941, 29947,   448, 29871, 29896, 29945,   353,   529, 29871, 29941,\n",
       "          29871, 29947,   448, 29871, 29896, 29871, 29945,   353, 29871, 29906,\n",
       "          29871, 29941,   869, 29871, 29900,  1405, 22831,  5520,  1135, 18888,\n",
       "          29915, 29879, 22378,    13,  4136, 29871, 29906, 29941,    13,    13,\n",
       "          32000]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from training.trainer import preprocess_dataset\n",
    "processed_dataset = preprocess_dataset(tokenizer=tokenizer, max_length=2048, seed=42, training_dataset=\"/home/mcwave/code/ChatGLM-6B/ptuning/word_problems/word_problems_train_data_2\")\n",
    "\n",
    "x=processed_dataset[0]\n",
    "x[\"input_ids\"]=torch.as_tensor([x[\"input_ids\"]])\n",
    "x[\"token_type_ids\"]=torch.as_tensor([x[\"token_type_ids\"]])\n",
    "x[\"attention_mask\"]=torch.as_tensor([x[\"attention_mask\"]])\n",
    "x[\"input_ids\"].to(model.device)\n",
    "x[\"token_type_ids\"].to(model.device)\n",
    "x[\"attention_mask\"].to(model.device)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[ 202378496,  202392192,  193629568,  133395872,  154827456,  155144736,\n",
      "          155141936,  133614656,  132658768,  192013104,  202576208,  193663280,\n",
      "          192268048,  202558896,  193593440,  193736560,  193848272,  133601056,\n",
      "          202632688,  130446912,  154805184,  202689616,  202376400,  202353712,\n",
      "          193682240,  193884672,  193638848,  193881632,  193724640,  202555248,\n",
      "          193891808,  154375104,  202575840,  190494080,  154797376,  154382448,\n",
      "          202558384,  193700256,  154828848,  154984880,  133626768,  130468064,\n",
      "          192093056,  154942704,  193701744,  154687664,  154908192,  154706368,\n",
      "          154328880,  154151552,  154424448,  155030624,  132671120,  145819024,\n",
      "          132681360,  145664320,  130473056,  132931472,  130323360,  130471968,\n",
      "          154947536,  145707648,  202629184,  202629440,  154325840,  154326096,\n",
      "          133610688,  133610944,  154229344,  154229600,  129538448,  129538704,\n",
      "          130358864,  130359120,  132634416,  132634672,  132657088,  132657344,\n",
      "          133296112,  133296368,  154959632,  154959888,  154966976,  154967232,\n",
      "          133320576,  133320832,  154698064,  154698320,  154906528,  154906784,\n",
      "          154148832,  154149088,  154768560,  154768816,  154776272,  154776528,\n",
      "          155134432,  155134688,  154717552,  154717808,  155116176,  155116432,\n",
      "          154953728,  154953984,  155019072,  155019328,  202629680,  202629936,\n",
      "          132648688,  132648944,  145072560,  145072816,  154287408,  154287616,\n",
      "          154358192,  154358448,  154357072,  154357280,  154909776,  154910032,\n",
      "          202579552,  202579760,  155048640,  155048896,  155045536,  155045792,\n",
      "          202628304,  202628560,  202628816,  155985024,  155985280,  155985536,\n",
      "          202540672,  202540928,  202541184,  202541440,  202630320,  202630576,\n",
      "          202630832,  202631088,  202627184,  202627440,  202627696,  202627952,\n",
      "          155979648,  155979904,  155980160,  155980416,  145817280,  145817536,\n",
      "          145817792,  145818048,  154996352,  154996608,  154996864,  154997120,\n",
      "          155971424,  155971680,  155971936,  155972192,  155972448,  155059904,\n",
      "          155060160,  155060416,  155060672,  155060928,  154998688,  154998944,\n",
      "          154999200,  154999456,  154999712,  202570432,  202570640,  202570896,\n",
      "          202571152,  202571408,  155016080,  155016336,  155016592,  155016848,\n",
      "          155017104,  155017360,  155035792,  155036048,  155036304,  155036560,\n",
      "          155036816,  155037072,  155992848,  155993104,  155993360,  155993616,\n",
      "          155993872,  155994128,  155011072,  155011328,  155011584,  155011840,\n",
      "          155012096,  155012352,  155065152,  155065408,  155065664,  155065920,\n",
      "          155066176,  155066432,  155989808,  155990064,  155990320,  155990576,\n",
      "          155990832,  155991088,  155991344,  155062736,  155062992,  155063248,\n",
      "          155063504,  155063760,  155064016,  155064272,  155982528,  155982784,\n",
      "          155983040,  155983296,  155983552,  155983808,  155984064,  155025984,\n",
      "          155026240,  155026496,  155026752,  155027008,  155027264,  155027520,\n",
      "          202615568,  202615824,  202616080,  202616336,  202616592,  202616848,\n",
      "          202617104,  202617360,  156108768,  156109024,  156109280,  156109536,\n",
      "          156109792,  156110048,  156110304,  156110560,  156110816,  156111072,\n",
      "          156111328,  156111584,  156111840,  156112096,  156112352,  156112608,\n",
      "          156000864,  156001120,  156001376,  156001632,  156001888,  156002144,\n",
      "          156002400,  156002656,  156002912,  156003168,  156003424,  156003680,\n",
      "          156003936,  156004192,  156004448,  156004704,  156004960,  156005216,\n",
      "          156005472,  156005728,  156005984,  156006240,  156006496,  156006752,\n",
      "          156007008,  156007264,  156007520,  156007776,  156008032,  156008288,\n",
      "          156008544,  156008800,  156009056,  156009312,  156009568,  156009824,\n",
      "          156010080,  156010336,  156010592,  156010848,  156011104,  156011360,\n",
      "          156011616,  156011872,  156012128,  156012384,  156012640,  156013104,\n",
      "               2496,        208, 140434962322312,          0,          1]],\n",
      "       device='cuda:0')\n",
      "1 tensor([[140435485094544, 140435485094544,  155043056,  155043056,  156012848,  133395872,\n",
      "          154827456,  155144736,  155141936,  133614656,  132658768,  192013104,\n",
      "          193629568,  155981808,  154123504,  202378496,  202392192,  202558896,\n",
      "          192268048,  193663280,  202576208,  154805184,  130446912,  202632688,\n",
      "          133601056,  193848272,  193736560,  193593440,  193884672,  193682240,\n",
      "          202353712,  202376400,  202689616,  155981504,  154375104,  193891808,\n",
      "          202555248,  193724640,  193881632,  193638848,  202575840,  154382448,\n",
      "          154797376,  190494080,  130468064,  133626768,  154984880,  154828848,\n",
      "          193700256,  202558384,  154942704,  192093056,  154687664,  193701744,\n",
      "          154328880,  154706368,  154908192,  132681360,  145819024,  132671120,\n",
      "          155030624,  154424448,  154151552,  130473056,  145664320,  154947536,\n",
      "          130471968,  130323360,  132931472,  145707648,  202629184,  202629440,\n",
      "          154229344,  154229600,  133610688,  133610944,  154325840,  154326096,\n",
      "          154966976,  154967232,  154959632,  154959888,  133296112,  133296368,\n",
      "          132657088,  132657344,  132634416,  132634672,  130358864,  130359120,\n",
      "          129538448,  129538704,  154906528,  154906784,  154698064,  154698320,\n",
      "          133320576,  133320832,  155019072,  155019328,  154953728,  154953984,\n",
      "          155116176,  155116432,  154717552,  154717808,  155134432,  155134688,\n",
      "          154776272,  154776528,  154768560,  154768816,  154148832,  154149088,\n",
      "          132648688,  132648944,  202629680,  202629936,  202579552,  202579808,\n",
      "          154909776,  154909984,  154357072,  154357328,  154358192,  154358400,\n",
      "          154287408,  154287664,  145072560,  145072768,  155045568,  155045824,\n",
      "          155048640,  155048896,  202628304,  202628560,  202628816,  155985024,\n",
      "          155985280,  155985536,  202540672,  202540928,  202541184,  202541440,\n",
      "          202630320,  202630576,  202630832,  202631088,  202627184,  202627440,\n",
      "          202627696,  202627952,  155979648,  155979904,  155980160,  155980416,\n",
      "          145817280,  145817536,  145817792,  145818048,  154996352,  154996608,\n",
      "          154996864,  154997120,  155971424,  155971680,  155971936,  155972192,\n",
      "          155972448,  155059904,  155060160,  155060416,  155060672,  155060928,\n",
      "          154998688,  154998944,  154999200,  154999456,  154999712,  202570432,\n",
      "          202570640,  202570896,  202571152,  202571408,  155016080,  155016336,\n",
      "          155016592,  155016848,  155017104,  155017360,  155035792,  155036048,\n",
      "          155036304,  155036560,  155036816,  155037072,  155992848,  155993104,\n",
      "          155993360,  155993616,  155993872,  155994128,  155011072,  155011328,\n",
      "          155011584,  155011840,  155012096,  155012352,  155065152,  155065408,\n",
      "          155065664,  155065920,  155066176,  155066432,  155989808,  155990064,\n",
      "          155990320,  155990576,  155990832,  155991088,  155991344,  155062736,\n",
      "          155062992,  155063248,  155063504,  155063760,  155064016,  155064272,\n",
      "          155025984,  155026240,  155026496,  155026752,  155027008,  155027264,\n",
      "          155027520,  202615568,  202615824,  202616080,  202616336,  202616592,\n",
      "          202616848,  202617104,  202617360,  155982064,  155982320,  155982576,\n",
      "          155982832,  155983088,  155983344,  155983600,  155983856,  155984112,\n",
      "          156000864,  156001120,  156001376,  156001632,  156001888,  156002144,\n",
      "          156002400,  156002656,  156002912,  156003168,  156003424,  156003680,\n",
      "          156003936,  156004192,  156004448,  156004704,  156004960,  156005216,\n",
      "          156005472,  156005728,  156005984,  156006240,  156006496,  156006752,\n",
      "          156007008,  156007264,  156007520,  156007776,  156008032,  156008288,\n",
      "          156008544,  156008800,  156009056,  156009312,  156009568,  156009824,\n",
      "          156010080,  156010336,  156010592,  156010848,  156011104,  156017600,\n",
      "          156017856,  156018112,  156018368,  156018624,  156018880,  156019136,\n",
      "          156019392,  156019648,  156019904,  156020160,  156020416,  156020672,\n",
      "          156020928,  156021184,  156021440,  156021696,       2496]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:691\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, token_type_ids)\u001b[0m\n\u001b[1;32m    688\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    690\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 691\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:580\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    572\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    573\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    574\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:289\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 289\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:85\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m     84\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m---> 85\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states)\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/home/mcwave/models/huggyllama-7b/llama-7b\")\n",
    " \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"/home/mcwave/models/huggyllama-7b/llama-7b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
      "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
      "         29889,    13,    13, 32001, 29871,    13, 29933,   300,  1017,   338,\n",
      "         14238,  6909,   363,   263,   716, 17042,  1026,   607, 21544,   395,\n",
      "         29896, 29900, 29900, 29889, 29782,   756,   871,  4203,   310,   278,\n",
      "          6909,  1183,  4225, 29889,  2439, 11825,  8459,   304,  2367,   902,\n",
      "           395, 29896, 29945,   363,   393,  6437, 29892,   322,   902,  4595,\n",
      "           862,  1237,  8951,   408,  1568,   408,   902, 11825, 29889,  1128,\n",
      "          1568,   901,  6909,   947, 29782,   817,   304, 15649,   278, 17042,\n",
      "          1026, 29973,    13,    13, 32002]], device='cuda:0') torch.Size([1, 95])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [74,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [326,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [65,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [324,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [73,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [348,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [349,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [336,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m instructions_3\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBetty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m instruction \u001b[38;5;129;01min\u001b[39;00m instructions_3:\n\u001b[0;32m---> 19\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response: \n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstruction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-----------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/word_problem_magnifier/training/generate.py:244\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(instruction, model, tokenizer, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given an instruction, uses the model and tokenizer to generate a response.  This formats the instruction in\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mthe instruction format that the model was fine-tuned on.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    str: response\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m generation_pipeline \u001b[38;5;241m=\u001b[39m InstructionTextGenerationPipeline(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/pipelines/base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1114\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         )\n\u001b[1;32m   1118\u001b[0m     )\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/pipelines/base.py:1127\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1126\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1127\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1128\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1025\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1026\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/word_problem_magnifier/training/generate.py:141\u001b[0m, in \u001b[0;36mInstructionTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     in_b \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#print(self.model)\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/generation/utils.py:1572\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1565\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1566\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1567\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/generation/utils.py:2619\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2619\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:689\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, token_type_ids)\u001b[0m\n\u001b[1;32m    686\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    702\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:289\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 289\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:85\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m     84\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m---> 85\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states)\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "instructions = [\n",
    "    \"Write a love letter to Edgar Allan Poe.\",\n",
    "    \"Write a tweet announcing Dolly, a large language model from Databricks.\",\n",
    "    \"I'm selling my Nikon D-750, write a short blurb for my ad.\",\n",
    "    \"Explain to me the difference between nuclear fission and fusion.\",\n",
    "    \"Give me a list of 5 science fiction books I should read next.\",\n",
    "] \n",
    "instructions_2 = [ \n",
    "    \"487 / 81\", \n",
    "    \"342 / 81\", \n",
    "    \"499 / 81\", \n",
    "    \"4295 / 81\", \n",
    "    \"9635 / 3711\"]\n",
    "instructions_3=[\n",
    "    \"Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\"\n",
    "]\n",
    "\n",
    "for instruction in instructions_3:\n",
    "    response = generate_response(instruction, model=model, tokenizer=tokenizer) \n",
    "    if response: \n",
    "        print(f\"Instruction: {instruction}\\n\\n{response}\\n\\n-----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "> initializing model parallel with size 2\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loading\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama/example.py\", line 119, in <module>\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama/example.py\", line 119, in <module>\n",
      "        fire.Fire(main)fire.Fire(main)\n",
      "\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)    \n",
      "component_trace = _Fire(component, args, parsed_flag_args, context, name)  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\n",
      "\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama/example.py\", line 78, in main\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama/example.py\", line 78, in main\n",
      "    generator = load(\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama/example.py\", line 48, in load\n",
      "    generator = load(\n",
      "  File \"/home/mcwave/code/word_problem_magnifier/llama/example.py\", line 48, in load\n",
      "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
      "FileNotFoundError    : with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:[Errno 2] No such file or directory: '/home/mcwave/models/huggyllama-7b/llama-7b/checkpoint-1000/params.json'\n",
      "\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/mcwave/models/huggyllama-7b/llama-7b/checkpoint-1000/params.json'\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 12188) of binary: /home/mcwave/anaconda3/envs/torch200/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/distributed/run.py\", line 762, in main\n",
      "    run(args)\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/distributed/run.py\", line 753, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "llama/example.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2023-06-30_21:17:28\n",
      "  host      : mcwave2\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 12189)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-06-30_21:17:28\n",
      "  host      : mcwave2\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 12188)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node 2 llama/example.py --ckpt_dir /home/mcwave/models/huggyllama-7b/llama-7b/checkpoint-1000 --tokenizer_path /home/mcwave/models/huggyllama-7b/llama-7b/checkpoint-1000/tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f=open(\"/home/mcwave/data/word_problems_test/word_problems_test.json\")\n",
    "data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "problems=data[\"foo\"]\n",
    "correct=0\n",
    "incorrect=0\n",
    "for i in range(100):\n",
    "    q=problems[i][\"question\"]\n",
    "    a=problems[i][\"answer\"]\n",
    "    correct_ans=\"\"\n",
    "    j=len(a)-1\n",
    "    while j>=0 and a[j]!=\" \":\n",
    "        correct_ans=a[j]+correct_ans\n",
    "        j-=1\n",
    "    print(\"Correct answer: \"+correct_ans)\n",
    "    response=generate_response(q,model=model,tokenizer=tokenizer)\n",
    "    res=\"\"\n",
    "    j=len(response)-1\n",
    "    while j>=0 and response[j] in \"0123456789.%\":\n",
    "        res=response[j]+res\n",
    "        j-=1\n",
    "    print(\"Model answer: \"+response)\n",
    "    outdict=problems[i]\n",
    "    outdict[\"model_res\"]=response\n",
    "    outdict[\"res\"]=correct_ans\n",
    "    if correct_ans==response:\n",
    "        correct+=1\n",
    "        with open(\"/home/mcwave/data/word_problems_test/word_problems_correct-falcon-7b-0625\",\"a\") as outfile:\n",
    "            json.dump(outdict,outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "    else:\n",
    "        incorrect+=1\n",
    "        with open(\"/home/mcwave/data/word_problems_test/word_problems_incorrect-falcon-7b-0625\",\"a\") as outfile:\n",
    "            json.dump(outdict,outfile)\n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 9 9 * 9 9 9 = < [ { ( 9 * 9 = 81 ) shifted 3 = 81000 } + { ( 9 * 9 = 81 ) shifted 2 = 8100 } + { ( 9 * 9 = 81 ) shifted 1 = 810 } + ( 9 * 9 = 81 ) = 89991 ] shifted 2 = 8999100 > + < [ { ( 9 * 9 = 81 ) shifted 3 = 81000 } + { ( 9 * 9 = 81 ) shifted 2 = 8100 } + { ( 9 * 9 = 81 ) shifted 1 = 810 } + ( 9 * 9 = 81 ) = 89991 ] shifted 1 = 899910 > + [ { ( 9 * 9 = 81 ) shifted 3 = 81000 } + { ( 9 * 9 = 81 ) shifted 2 = 8100 } + { ( 9 * 9 = 81 ) shifted 1 = 810 } + ( 9 * 9 = 81 ) = 89991 ] = 8 9 9 9 1 0 0 + 8 9 9 9 1 0 + 8 9 9 9 1 = 9 8 9 9 0 1 0 + 8 9 9 9 1 = 9 9 8 9 0 0 1 \n"
     ]
    }
   ],
   "source": [
    "print(\"9 9 9 9 * 9 9 9 = \"+to_words_mult(\"9999\",\"999\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_eval_set(x):\n",
    "    res=[]\n",
    "    for i in range(x):\n",
    "        op=random.randint(1,4)\n",
    "        if op==1:\n",
    "            digits_1=random.randint(2,4)\n",
    "            digits_2=random.randint(1,digits_1)\n",
    "            a=random.randint(10**(digits_1-1),(10**digits_1)-1)\n",
    "            b=random.randint(max(10**(digits_2-1),1),min((10**digits_2)-1,a))\n",
    "            res.append(str(a)+\" / \"+str(b))\n",
    "        if op==2:\n",
    "            a=random.randrange(2,9999)\n",
    "            b=random.randrange(2,999)\n",
    "            res.append(str(a)+\" * \"+str(b))\n",
    "        if op==3:\n",
    "            a=random.randrange(2,9999)\n",
    "            b=random.randrange(2,9999)\n",
    "            res.append(str(a)+\" + \"+str(b))\n",
    "        if op==4:\n",
    "            a=random.randrange(3,9999)\n",
    "            b=random.randrange(2,a)\n",
    "            res.append(str(a)+\" - \"+str(b))\n",
    "    return res\n",
    "eval_set=get_eval_set(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "343 - 138\n",
      "3 4 3 - 1 3 8 = 343 - 138 = 3 4 3 - 1 3 8 = 1 0 5\n",
      "2 0 5\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "7089 * 635\n",
      "7 0 8 9 * 6 3 5 = < [ { ( 7 * 6 = 42 ) shifted 3 = 42000 } + { ( 0 * 6 = 0 ) shifted 2 = 0 } + { ( 8 * 6 = 48 ) shifted 1 = 480 } + ( 9 * 6 = 54 ) = 42534 ] shifted 2 = 4253400 > + < [ { ( 7 * 3 = 21 ) shifted 3 = 21000 } + { ( 0 * 3 = 0 ) shifted 2 = 0 } + { ( 8 * 3 = 24 ) shifted 1 = 240 } + ( 9 * 3 = 27 ) = 21267 ] shifted 1 = 212670 > + [ { ( 7 * 5 = 35 ) shifted 3 = 35000 } + { ( 0 * 5 = 0 ) shifted 2 = 0 } + { ( 8 * 5 = 40 ) shifted 1 = 400 } + ( 9 * 5 = 45 ) = 35445 ] = 4 2 5 3 4 0 0 + 2 1 2 6 7 0 + 3 5 4 4 5 = 4 4 6 6 0 7 0 + 3 5 4 4 5 = 4 4 0 1 5 1 5\n",
      "4 5 0 1 5 1 5\n",
      "80\n",
      "640 / 46\n",
      "640 / 46 = 6 4 0 / 4 6 = < [ { ( 6 4 / 4 6 # because 1 * 4 6 <= 6 4 | 1, remainder = 6 4 - 1 * 4 6 = 6 4 - 4 6 = 1 8 ) shifted 1 = 1 0 } + { ( 1 8 shifted 1 + 0 = 1 8 0 / 4 6 # because 4 * 4 6 <= 1 8 0 | 4, remainder = 1 8 0 - 4 * 4 6 = 1 8 0 - 1 8 4 = 6 ) = 4 } ] = 1 0 + 4 = 1 4 > = 1 4 remainder 6\n",
      "1 3 remainder 4 2\n",
      "90\n",
      "4191 / 683\n",
      "4191 / 683 = 4 1 9 1 / 6 8 3 = < [ { ( 4 1 9 1 / 6 8 3 # because 5 * 6 8 3 <= 4 1 9 1 | 5, remainder = 4 1 9 1 - 5 * 6 8 3 = 4 1 9 1 - 3 4 1 5 = 7 7 6 ) = 5 } ] = 5 > = 5 remainder 7 7 6\n",
      "6 remainder 9 3\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def to_ans(s):\n",
    "    number_1=\"\"\n",
    "    i=0\n",
    "    while s[i]!=\" \":\n",
    "        number_1+=s[i]\n",
    "        i+=1\n",
    "    symbol=s[i+1]\n",
    "    is_div=False\n",
    "    if s[i+1]==\"/\":\n",
    "        is_div=True\n",
    "    i+=3\n",
    "    number_2=\"\"\n",
    "    while i<len(s):\n",
    "        number_2+=s[i]\n",
    "        i+=1\n",
    "    res=space(str(math.floor(eval(s))))\n",
    "    if is_div:\n",
    "        res+=\" remainder \"+space(str(int(number_1)%int(number_2)))\n",
    "    return res\n",
    "def eval_model(model,tokenizer,eval_set):\n",
    "    correct=0\n",
    "    correct_cases=[]\n",
    "    incorrect_cases=[[],[],[],[]]\n",
    "    num_checked=0\n",
    "    sym2idx={\"+\":0,\"-\":1,\"*\":2,\"/\":3}\n",
    "    for instruction in eval_set[200:]:\n",
    "        response = generate_response(instruction, model=model, tokenizer=tokenizer)\n",
    "        it=len(response)-1\n",
    "        while response[it]!=\"=\":\n",
    "            it-=1\n",
    "        response_num=response[it+2:len(response)]\n",
    "        if response_num==to_ans(instruction):\n",
    "            correct+=1\n",
    "            correct_cases.append(instruction)\n",
    "        else:\n",
    "            symbol=\"\"\n",
    "            i=0\n",
    "            while instruction[i] not in \"+-*/\":\n",
    "                i+=1\n",
    "            symbol=instruction[i]\n",
    "            incorrect_cases[sym2idx[symbol]].append(instruction)\n",
    "            print(instruction)\n",
    "            print(response)\n",
    "            i=0\n",
    "            number=\"\"\n",
    "            while instruction[i]!=\" \":\n",
    "                number+=instruction[i]\n",
    "                i+=1\n",
    "            print(to_ans(instruction))\n",
    "        if num_checked%10==0:\n",
    "            print(num_checked)\n",
    "        if num_checked==100:\n",
    "            break\n",
    "        num_checked+=1\n",
    "    return correct_cases,incorrect_cases\n",
    "correct,incorrect=eval_model(model,tokenizer,eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['343 - 138'], ['7089 * 635'], ['640 / 46', '4191 / 683']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2584, 43),\n",
       " (7357, 36),\n",
       " (8406, 47),\n",
       " (5834, 592),\n",
       " (8263, 46),\n",
       " (925, 61),\n",
       " (768, 32),\n",
       " (446, 49),\n",
       " (1918, 785)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction=[to_words(123,456)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((1 times 4) shifted 2 + (2 times 4) shifted 1 + (3 times 4)) shifted 2 + ((1 times 5) shifted 2 + (2 times 5) shifted 1 + (3 times 5)) shifted 1 + ((1 times 6) shifted 2 + (2 times 6) shifted 1 + (3 times 6)) shifted 0 equals 56088'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_words(\"123\",\"456\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mcwave/data/word_problems_test/word_problems_test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_prompt\u001b[39m(question):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor the question \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mquestion\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m , multiply all numbers in the question by 2, then tell me the modified question and the final number in its answer. Do not say anything else.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f=open(\"/home/mcwave/data/word_problems_test/word_problems_test.json\")\n",
    "data=json.load(f)\n",
    "\n",
    "import openai\n",
    "\n",
    "def to_prompt(question):\n",
    "    return \"For the question \"+question+\" , multiply all numbers in the question by 2, then tell me the modified question and the final number in its answer. Do not say anything else.\"\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "    \n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "    \n",
    "    def execute(self):\n",
    "        completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.messages)\n",
    "        # Uncomment this to print out token usage each time, e.g.\n",
    "        # {\"completion_tokens\": 86, \"prompt_tokens\": 26, \"total_tokens\": 112}\n",
    "        # print(completion.usage)\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "openai.api_key=\"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\"\n",
    "verifier=ChatBot()\n",
    "prompt=to_prompt('In the city of Soda, there are exactly 237860 inhabitants.They include 84170 men and 90920 women.The rest of the population is made up of children.How many kids are there in Soda?')\n",
    "verifier(prompt)\n",
    "problems=data[\"problems\"]\n",
    "for i in range(len(problems)):\n",
    "    with open(\"word_problems/word_problems_train_new.jsonl\", \"a\") as outfile:\n",
    "        output=verifier(to_prompt(problems[i][\"question\"]))\n",
    "        output=output[19:]\n",
    "        it=0\n",
    "        real_output={\"question\":\"\",\"answer\":\"\"}\n",
    "        while output[it]!=\"\\n\":\n",
    "            real_output[\"question\"]+=output[it]\n",
    "            it+=1\n",
    "        real_output[\"answer\"]+=output[it+16:]\n",
    "        json.dump(real_output,outfile)\n",
    "    if i%100==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modified question: Natalia sold clips to 480 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\n\\nFinal answer: 5040.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=to_prompt('Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?')\n",
    "verifier(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"problems\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f=open(\"/home/mcwave/data/word_problems/word_problems_train_data_2/word_problems_train_data.jsonl\")\n",
    "data=json.load(f)\n",
    "with open(\"/home/mcwave/data/word_problems/word_problems_train_data_2/word_problems_train_data_2.jsonl\", \"a\") as outfile:\n",
    "    outfile.write(\"\\n\")\n",
    "for question in data:\n",
    "    with open(\"/home/mcwave/data/word_problems/word_problems_train_data_2/word_problems_train_data_2.jsonl\", \"a\") as outfile:\n",
    "        new_q={\"instruction\":question[\"question\"], \"context\": \"\", \"category\": \"open_qa\", \"response\": question[\"answer\"]}\n",
    "        json.dump(new_q,outfile)\n",
    "        outfile.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch200]",
   "language": "python",
   "name": "conda-env-torch200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
