{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-19 21:03:43--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 99836756 (95M) [application/x-deb]\n",
      "Saving to: ‘/tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb’\n",
      "\n",
      "parse-dev-11-3_11.5  38%[======>             ]  36.63M   728KB/s    eta 74s    ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb -O /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb && \\\n",
    "  wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-3_11.5.1.109-1_amd64.deb -O /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb && \\\n",
    "  wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb -O /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb && \\\n",
    "  wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-3_10.2.4.109-1_amd64.deb -O /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb && \\\n",
    "  dpkg -i /tmp/libcusparse-dev-11-3_11.5.0.58-1_amd64.deb && \\\n",
    "  dpkg -i /tmp/libcublas-dev-11-3_11.5.1.109-1_amd64.deb && \\\n",
    "  dpkg -i /tmp/libcusolver-dev-11-3_11.1.2.109-1_amd64.deb && \\\n",
    "  dpkg -i /tmp/libcurand-dev-11-3_10.2.4.109-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def space(a):\n",
    "    res=\"\"\n",
    "    for i in range(len(a)):\n",
    "        res+=a[i]\n",
    "        if i!=len(a)-1:\n",
    "            res+=\" \"\n",
    "    return res\n",
    "def to_words_mult(a,b):\n",
    "    res=\"\"\n",
    "    for i in range(len(b)):\n",
    "        if i!=len(b)-1:\n",
    "            res+=\"< \"\n",
    "        res+=\"[ \"\n",
    "        for j in range(len(a)):\n",
    "            if j!=len(a)-1:\n",
    "                res+=\"{ \"\n",
    "            if len(a)-j>1:\n",
    "                res+=\"( \"+a[j]+\" * \"+b[i]+\" = \"+str(int(a[j])*int(b[i]))+\" ) \"\n",
    "                res+=\"shifted \"+str(len(a)-j-1)\n",
    "                res+=\" = \"+str(int(a[j])*int(b[i])*10**(len(a)-j-1))\n",
    "            else:\n",
    "                res+=\"( \"+a[j]+\" * \"+b[i]+\" = \"+str(int(a[j])*int(b[i]))+\" )\"\n",
    "            if j!=len(a)-1:\n",
    "                res+=\" }\"\n",
    "            if j!=len(a)-1:\n",
    "                res+=\" + \"\n",
    "        res+=\" = \"+str(int(a)*int(b[i]))\n",
    "        res+=\" ] \"\n",
    "        if len(b)-i>1:\n",
    "            res+=\"shifted \"+str(len(b)-i-1) \n",
    "            res+=\" = \"+str(int(a)*int(b[i])*10**(len(b)-i-1))\n",
    "            res+=\" \"\n",
    "        if i!=len(b)-1:\n",
    "            res+=\">\"\n",
    "        if i!=len(b)-1:\n",
    "            res+=\" + \"\n",
    "    res+=\"= \"\n",
    "    nums=[]\n",
    "    for i in range(len(b)):\n",
    "        num=str(int(a)*int(b[i])*10**(len(b)-i-1))\n",
    "        for j in str(int(a)*int(b[i])*10**(len(b)-i-1)):\n",
    "            res+=j+\" \"\n",
    "        nums.append(num)\n",
    "        if i!=len(b)-1:\n",
    "            res+=\"+ \"\n",
    "    sum=int(nums[0])\n",
    "    for i in range(1,len(nums)):\n",
    "        sum+=int(nums[i])\n",
    "        if i==len(nums)-1:\n",
    "            res+=\"= \"\n",
    "            for j in str(int(a)*int(b)):\n",
    "                res+=j+\" \"\n",
    "            break\n",
    "        res+=\"= \"\n",
    "        for j in str(sum):\n",
    "            res+=j+\" \"\n",
    "        if i<len(nums)-1:\n",
    "            res+=\"+ \"\n",
    "            for j in range(i+1,len(nums)):\n",
    "                for k in nums[j]:\n",
    "                    res+=k+\" \"\n",
    "                if j!=len(nums)-1:\n",
    "                    res+=\"+\"\n",
    "    return res\n",
    "def to_words_div(a,b):\n",
    "    res=\"\"\n",
    "    res+=space(a)+\" / \"+space(b)\n",
    "    res+=\" =\"\n",
    "    rem=int(a)\n",
    "    res+=\" < \"\n",
    "    res+=\"[\"\n",
    "    mid_reses=[]\n",
    "    first=True\n",
    "    prev_remainder=-1\n",
    "    while rem>=int(b):\n",
    "        digits=1\n",
    "        while(int(str(rem)[:digits])<int(b)):\n",
    "            digits+=1\n",
    "        mid_res=math.floor(float(str(rem)[:digits])/float(b))\n",
    "        res+=\" { \"\n",
    "        res+=\"( \"\n",
    "        tmp=prev_remainder\n",
    "        prev_remainder=float(str(rem)[:digits])-mid_res*float(b)\n",
    "        if not first:\n",
    "            res+=space(str(tmp))+\" shifted 1\"+\" + \"+str(rem)[digits-1]+\" = \"\n",
    "        res+=space(str(rem)[:digits])+\" / \"+space(b)+\" #\"\n",
    "        res+=\" because \"+str(mid_res)+\" * \"+space(b)+\" <= \"+space(str(rem)[:digits])\n",
    "        res+=\" |\"\n",
    "        res+=\" \"+str(mid_res)+\",\"\n",
    "        res+=\" remainder \"+\"= \"+space(str(rem)[:digits])+\" - \"+str(mid_res)+\" * \"+space(b)+\" = \"+space(str(rem)[:digits])+\" - \"+space(str(mid_res*int(b)))+\" = \"+space(str(prev_remainder))+\" )\"\n",
    "        if len(str(rem))-digits>0:\n",
    "            res+=\" shifted \"+str(len(str(rem))-digits)\n",
    "        res+=\" = \"+space(str(mid_res*10**(len(str(rem))-digits)))\n",
    "        mid_reses.append(mid_res*(10**(len(str(rem))-digits)))\n",
    "        rem-=int(b)*mid_res*(10**(len(str(rem))-digits))\n",
    "        res+=\" }\"\n",
    "        if rem>=int(b):\n",
    "            res+=\" +\"  \n",
    "        first=False\n",
    "    res+=\" ]\"\n",
    "    sum=0\n",
    "    for i in range(len(mid_reses)):\n",
    "        sum+=int(mid_reses[i])\n",
    "        res+=\" = \"\n",
    "        res+=space(str(sum))\n",
    "        if i!=len(mid_reses)-1:\n",
    "            res+=\" + \"\n",
    "        for j in range(i+1,len(mid_reses)):\n",
    "            res+=space(str(mid_reses[j]))\n",
    "            if j!=len(mid_reses)-1:\n",
    "                res+=\" + \"\n",
    "    res+=\" >\"\n",
    "    res+=\" = \"+space(str(int(int(a)/int(b))))+\" remainder \"+space(str(rem))\n",
    "    return res\n",
    "def to_words_add(a,b):\n",
    "    res=\"\"\n",
    "    #res+=a+\" + \"+b+\" = \"\n",
    "    a_spaced=space(a)\n",
    "    b_spaced=space(b)\n",
    "    res+=a_spaced+\" + \"+b_spaced+\" = \"\n",
    "    res+=space(str(int(a)+int(b)))\n",
    "    return res\n",
    "def to_words_sub(a,b):\n",
    "    res=\"\"\n",
    "    #res+=a+\" - \"+b+\" = \"\n",
    "    a_spaced=space(a)    \n",
    "    b_spaced=space(b)\n",
    "    res+=a_spaced+\" - \"+b_spaced+\" = \"\n",
    "    res+=space(str(int(a)-int(b)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "with open(\"databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "    outfile.write(\"\\n\")\n",
    "for i in range(120000):\n",
    "    operation=random.randint(1,6)\n",
    "    if str(operation) in \"12\":\n",
    "        a=random.randrange(2,9999)\n",
    "        b=random.randrange(2,9999)\n",
    "        instructions=str(a)+\" * \"+str(b)\n",
    "        response=\"\"\n",
    "        for char in str(a):\n",
    "            response+=char+\" \"\n",
    "        response+=\"* \"\n",
    "        for char in str(b):\n",
    "            response+=char+\" \"\n",
    "        response+=\"= \"+to_words_mult(str(a),str(b))\n",
    "        question={\"instruction\":instructions, \"context\": \"\", \"category\": \"open_qa\", \"response\": response}\n",
    "        with open(\"multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "            json.dump(question, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "    if str(operation) in \"34\":\n",
    "        digits_1=random.randint(2,4)\n",
    "        digits_2=random.randint(1,digits_1)\n",
    "        a=random.randint(10**(digits_1-1),(10**digits_1)-1)\n",
    "        b=random.randint(max(10**(digits_2-1),1),min((10**digits_2)-1,a))\n",
    "        instructions=str(a)+\" / \"+str(b)\n",
    "        response=\"\"\n",
    "        response+=to_words_div(str(a),str(b))\n",
    "        question={\"instruction\":instructions, \"context\": \"\", \"category\": \"open_qa\", \"response\": response}\n",
    "        with open(\"multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "            json.dump(question, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "    if operation==5:\n",
    "        a=random.randrange(2,9999)\n",
    "        b=random.randrange(2,9999)\n",
    "        instructions=str(a)+\" + \"+str(b)\n",
    "        response=\"\"\n",
    "        for char in str(a):\n",
    "            response+=char+\" \"\n",
    "        response+=\"+ \"\n",
    "        for char in str(b):\n",
    "            response+=char+\" \"\n",
    "        response+=\"= \"+to_words_add(str(a),str(b))\n",
    "        question={\"instruction\":instructions, \"context\": \"\", \"category\": \"open_qa\", \"response\": response}\n",
    "        with open(\"multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "            json.dump(question, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "    if operation==6:\n",
    "        a=random.randrange(3,9999)\n",
    "        b=random.randrange(2,a)\n",
    "        instructions=str(a)+\" - \"+str(b)\n",
    "        response=\"\"\n",
    "        for char in str(a):\n",
    "            response+=char+\" \"\n",
    "        response+=\"- \"\n",
    "        for char in str(b):\n",
    "            response+=char+\" \"\n",
    "        response+=\"= \"+to_words_sub(str(a),str(b))\n",
    "        question={\"instruction\":instructions, \"context\": \"\", \"category\": \"open_qa\", \"response\": response}\n",
    "        with open(\"multiplication_data/databricks-dolly-15k.jsonl\", \"a\") as outfile:\n",
    "            json.dump(question, outfile)\n",
    "            outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>318556</td>\n",
       "      <td>48</td>\n",
       "      <td>A very clean and well decorated empty bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116100</td>\n",
       "      <td>67</td>\n",
       "      <td>A panoramic view of a kitchen and all of its a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>318556</td>\n",
       "      <td>126</td>\n",
       "      <td>A blue and white bathroom with butterfly theme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116100</td>\n",
       "      <td>148</td>\n",
       "      <td>A panoramic photo of a kitchen and dining room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>379340</td>\n",
       "      <td>173</td>\n",
       "      <td>A graffiti-ed stop sign across the street from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id   id                                            caption\n",
       "0    318556   48     A very clean and well decorated empty bathroom\n",
       "1    116100   67  A panoramic view of a kitchen and all of its a...\n",
       "2    318556  126  A blue and white bathroom with butterfly theme...\n",
       "3    116100  148     A panoramic photo of a kitchen and dining room\n",
       "4    379340  173  A graffiti-ed stop sign across the street from..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "import pandas as pd\n",
    "aa = pd.read_csv('/home/mcwave/data/data_file.csv')\n",
    "aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 20:18:42 INFO [numexpr.utils] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-06-17 20:18:42 INFO [numexpr.utils] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"sh.command\").setLevel(logging.ERROR)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import training.consts as consts\n",
    "from datetime import datetime\n",
    "from training.consts import DEFAULT_INPUT_MODEL, SUGGESTED_INPUT_MODELS\n",
    "from training.trainer import load_training_dataset, load_tokenizer\n",
    "# COMMAND ----------\n",
    "\n",
    "# Cache data and tokenizer locally before creating a bunch of deepspeed processes and make sure they succeeds.\n",
    "#load_training_dataset(\"/home/mcwave/data/databricks-dolly-15k\")\n",
    "#load_training_dataset(\"/home/mcwave/data/finetune_data_dolly\")\n",
    "#print(\"dataset loaded\")\n",
    "#load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Output Dir: /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems/\n",
      "DBFS Output Dir: /home/mcwave/data/results/model\n",
      "Tensorboard Display Dir: /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems//runs\n",
      "[2023-06-17 22:07:28,206] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-06-17 22:07:28,218] [INFO] [runner.py:541:main] cmd = /home/mcwave/anaconda3/envs/torch200/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --module --enable_each_rank_log=None training.trainer --input-model /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems --deepspeed /home/mcwave/data/config/ds_z3_bf16_config.json --epochs 2 --local-output-dir /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems/ --dbfs-output-dir /home/mcwave/data/results/model --per-device-train-batch-size 6 --per-device-eval-batch-size 6 --logging-steps 10 --save-steps 500 --save-total-limit 20 --eval-steps 50 --warmup-steps 50 --test-size 200 --lr 5e-6\n",
      "[2023-06-17 22:07:29,664] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1]}\n",
      "[2023-06-17 22:07:29,664] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=2, node_rank=0\n",
      "[2023-06-17 22:07:29,664] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n",
      "[2023-06-17 22:07:29,664] [INFO] [launch.py:247:main] dist_world_size=2\n",
      "[2023-06-17 22:07:29,664] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "Input model:/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems\n",
      "2023-06-17 22:07:31 INFO [__main__] Loading tokenizer for /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems\n",
      "Input model:/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems\n",
      "2023-06-17 22:07:31 INFO [__main__] Loading tokenizer for /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems\n",
      "2023-06-17 22:07:31 INFO [__main__] Loading model for /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems\n",
      "2023-06-17 22:07:31 INFO [__main__] Loading model for /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems\n",
      "2023-06-17 22:07:51 INFO [__main__] Found max lenth: 2048\n",
      "2023-06-17 22:07:51 INFO [__main__] Loading dataset from /home/mcwave/data/word_problems\n",
      "/home/mcwave/data/word_problems\n",
      "2023-06-17 22:07:51 WARNING [datasets.builder] Found cached dataset json (/home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 954.77it/s]\n",
      "2023-06-17 22:07:51 INFO [__main__] Found 7473 rows\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-20074b6130c6e2b0.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Preprocessing dataset\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f22d844f37942fbb.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Processed dataset has 7473 rows\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-642eec1d517b2861.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Processed dataset has 7473 rows after filtering for truncated records\n",
      "2023-06-17 22:07:51 INFO [__main__] Shuffling dataset\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached shuffled indices for dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-17a6f099d3a39a11.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Done preprocessing\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached split indices for dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fe91c564c7daf4fa.arrow and /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-d51abf7712351308.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Train data size: 7273\n",
      "2023-06-17 22:07:51 INFO [__main__] Test data size: 200\n",
      "[2023-06-17 22:07:51,853] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "2023-06-17 22:07:51 INFO [__main__] Found max lenth: 2048\n",
      "2023-06-17 22:07:51 INFO [__main__] Loading dataset from /home/mcwave/data/word_problems\n",
      "/home/mcwave/data/word_problems\n",
      "2023-06-17 22:07:51 WARNING [datasets.builder] Found cached dataset json (/home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1033.33it/s]\n",
      "2023-06-17 22:07:51 INFO [__main__] Found 7473 rows\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-20074b6130c6e2b0.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Preprocessing dataset\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f22d844f37942fbb.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Processed dataset has 7473 rows\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-642eec1d517b2861.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Processed dataset has 7473 rows after filtering for truncated records\n",
      "2023-06-17 22:07:51 INFO [__main__] Shuffling dataset\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached shuffled indices for dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-17a6f099d3a39a11.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Done preprocessing\n",
      "2023-06-17 22:07:51 WARNING [datasets.arrow_dataset] Loading cached split indices for dataset at /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fe91c564c7daf4fa.arrow and /home/mcwave/.cache/huggingface/datasets/json/word_problems-7aca4ed69c824f8e/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-d51abf7712351308.arrow\n",
      "2023-06-17 22:07:51 INFO [__main__] Train data size: 7273\n",
      "2023-06-17 22:07:51 INFO [__main__] Test data size: 200\n",
      "2023-06-17 22:07:52 INFO [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "2023-06-17 22:07:52 INFO [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-06-17 22:07:52 INFO [torch.distributed.distributed_c10d] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "init\n",
      "Config {'bf16': {'enabled': 'auto'}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'scheduler': {'type': 'WarmupLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto'}}, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': 2000, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False}\n",
      "Training args TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=/home/mcwave/data/config/ds_z3_bf16_config.json,\n",
      "disable_tqdm=True,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems//runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=6,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=20,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=50,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "2023-06-17 22:07:52 INFO [__main__] Instantiating Trainer\n",
      "2023-06-17 22:07:52 INFO [__main__] Training\n",
      "2023-06-17 22:07:52 INFO [torch.distributed.distributed_c10d] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "init\n",
      "Config {'bf16': {'enabled': 'auto'}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'scheduler': {'type': 'WarmupLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto'}}, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': 2000, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False}\n",
      "Training args TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=/home/mcwave/data/config/ds_z3_bf16_config.json,\n",
      "disable_tqdm=True,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems//runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=6,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=20,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=50,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "2023-06-17 22:07:52 INFO [__main__] Instantiating Trainer\n",
      "2023-06-17 22:07:52 INFO [__main__] Training\n",
      "2023-06-17 22:07:56 INFO [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "2023-06-17 22:07:56 INFO [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-06-17 22:07:56 INFO [torch.distributed.distributed_c10d] Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "2023-06-17 22:07:56 INFO [torch.distributed.distributed_c10d] Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "Using /home/mcwave/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Using /home/mcwave/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/mcwave/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.2852590084075928 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.2723641395568848 seconds\n",
      "Using /home/mcwave/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Using /home/mcwave/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/mcwave/.cache/torch_extensions/py310_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.05605936050415039 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10138869285583496 seconds\n",
      "Rank: 0 partition count [2] and sizes[(1387543040, False)] \n",
      "Rank: 1 partition count [2] and sizes[(1387543040, False)] \n",
      "Using /home/mcwave/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.04982900619506836 seconds\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Using /home/mcwave/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00402522087097168 seconds\n",
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 7.6906, 'learning_rate': 2.9429595503388954e-06, 'epoch': 0.02}\n",
      "{'loss': 3.9375, 'learning_rate': 3.8288786510166845e-06, 'epoch': 0.03}\n",
      "{'loss': 2.6027, 'learning_rate': 4.347108103585803e-06, 'epoch': 0.05}\n",
      "{'loss': 1.6359, 'learning_rate': 4.714797751694474e-06, 'epoch': 0.07}\n",
      "{'loss': 1.6848, 'learning_rate': 5e-06, 'epoch': 0.08}\n",
      "{'eval_loss': 1.5876562595367432, 'eval_runtime': 1.2264, 'eval_samples_per_second': 163.075, 'eval_steps_per_second': 13.861, 'epoch': 0.08}\n",
      "{'loss': 1.6414, 'learning_rate': 5e-06, 'epoch': 0.1}\n",
      "{'loss': 1.7488, 'learning_rate': 5e-06, 'epoch': 0.12}\n",
      "{'loss': 1.6023, 'learning_rate': 5e-06, 'epoch': 0.13}\n",
      "{'loss': 1.5621, 'learning_rate': 5e-06, 'epoch': 0.15}\n",
      "{'loss': 1.5973, 'learning_rate': 5e-06, 'epoch': 0.16}\n",
      "{'eval_loss': 1.59375, 'eval_runtime': 1.2278, 'eval_samples_per_second': 162.894, 'eval_steps_per_second': 13.846, 'epoch': 0.16}\n",
      "{'loss': 1.5184, 'learning_rate': 5e-06, 'epoch': 0.18}\n",
      "{'loss': 1.5961, 'learning_rate': 5e-06, 'epoch': 0.2}\n",
      "{'loss': 1.4762, 'learning_rate': 5e-06, 'epoch': 0.21}\n",
      "{'loss': 1.4691, 'learning_rate': 5e-06, 'epoch': 0.23}\n",
      "{'loss': 1.5262, 'learning_rate': 5e-06, 'epoch': 0.25}\n",
      "{'eval_loss': 1.5057421922683716, 'eval_runtime': 1.2443, 'eval_samples_per_second': 160.731, 'eval_steps_per_second': 13.662, 'epoch': 0.25}\n",
      "{'loss': 1.5934, 'learning_rate': 5e-06, 'epoch': 0.26}\n",
      "{'loss': 1.5961, 'learning_rate': 5e-06, 'epoch': 0.28}\n",
      "{'loss': 1.5441, 'learning_rate': 5e-06, 'epoch': 0.3}\n",
      "{'loss': 1.5451, 'learning_rate': 5e-06, 'epoch': 0.31}\n",
      "{'loss': 1.6227, 'learning_rate': 5e-06, 'epoch': 0.33}\n",
      "{'eval_loss': 1.454531192779541, 'eval_runtime': 1.3281, 'eval_samples_per_second': 150.596, 'eval_steps_per_second': 12.801, 'epoch': 0.33}\n",
      "{'loss': 1.5176, 'learning_rate': 5e-06, 'epoch': 0.35}\n",
      "{'loss': 1.4496, 'learning_rate': 5e-06, 'epoch': 0.36}\n",
      "{'loss': 1.5395, 'learning_rate': 5e-06, 'epoch': 0.38}\n",
      "{'loss': 1.5543, 'learning_rate': 5e-06, 'epoch': 0.4}\n",
      "{'loss': 1.5437, 'learning_rate': 5e-06, 'epoch': 0.41}\n",
      "{'eval_loss': 1.4723827838897705, 'eval_runtime': 1.228, 'eval_samples_per_second': 162.87, 'eval_steps_per_second': 13.844, 'epoch': 0.41}\n",
      "{'loss': 1.5055, 'learning_rate': 5e-06, 'epoch': 0.43}\n",
      "{'loss': 1.507, 'learning_rate': 5e-06, 'epoch': 0.44}\n",
      "{'loss': 1.5234, 'learning_rate': 5e-06, 'epoch': 0.46}\n",
      "{'loss': 1.5926, 'learning_rate': 5e-06, 'epoch': 0.48}\n",
      "{'loss': 1.5578, 'learning_rate': 5e-06, 'epoch': 0.49}\n",
      "{'eval_loss': 1.4947656393051147, 'eval_runtime': 1.3016, 'eval_samples_per_second': 153.66, 'eval_steps_per_second': 13.061, 'epoch': 0.49}\n",
      "{'loss': 1.5383, 'learning_rate': 5e-06, 'epoch': 0.51}\n",
      "{'loss': 1.5168, 'learning_rate': 5e-06, 'epoch': 0.53}\n",
      "{'loss': 1.5301, 'learning_rate': 5e-06, 'epoch': 0.54}\n",
      "{'loss': 1.4383, 'learning_rate': 5e-06, 'epoch': 0.56}\n",
      "{'loss': 1.475, 'learning_rate': 5e-06, 'epoch': 0.58}\n",
      "{'eval_loss': 1.4672656059265137, 'eval_runtime': 1.2253, 'eval_samples_per_second': 163.223, 'eval_steps_per_second': 13.874, 'epoch': 0.58}\n",
      "{'loss': 1.4783, 'learning_rate': 5e-06, 'epoch': 0.59}\n",
      "{'loss': 1.4625, 'learning_rate': 5e-06, 'epoch': 0.61}\n",
      "{'loss': 1.4586, 'learning_rate': 5e-06, 'epoch': 0.63}\n",
      "{'loss': 1.4816, 'learning_rate': 5e-06, 'epoch': 0.64}\n",
      "{'loss': 1.4625, 'learning_rate': 5e-06, 'epoch': 0.66}\n",
      "{'eval_loss': 1.455234408378601, 'eval_runtime': 1.398, 'eval_samples_per_second': 143.065, 'eval_steps_per_second': 12.161, 'epoch': 0.66}\n",
      "{'loss': 1.4484, 'learning_rate': 5e-06, 'epoch': 0.68}\n",
      "{'loss': 1.4789, 'learning_rate': 5e-06, 'epoch': 0.69}\n",
      "{'loss': 1.541, 'learning_rate': 5e-06, 'epoch': 0.71}\n",
      "{'loss': 1.6035, 'learning_rate': 5e-06, 'epoch': 0.72}\n",
      "{'loss': 1.5193, 'learning_rate': 5e-06, 'epoch': 0.74}\n",
      "{'eval_loss': 1.4362109899520874, 'eval_runtime': 1.2506, 'eval_samples_per_second': 159.929, 'eval_steps_per_second': 13.594, 'epoch': 0.74}\n",
      "{'loss': 1.534, 'learning_rate': 5e-06, 'epoch': 0.76}\n",
      "{'loss': 1.482, 'learning_rate': 5e-06, 'epoch': 0.77}\n",
      "{'loss': 1.4281, 'learning_rate': 5e-06, 'epoch': 0.79}\n",
      "{'loss': 1.5051, 'learning_rate': 5e-06, 'epoch': 0.81}\n",
      "{'loss': 1.4988, 'learning_rate': 5e-06, 'epoch': 0.82}\n",
      "{'eval_loss': 1.415468692779541, 'eval_runtime': 1.2629, 'eval_samples_per_second': 158.37, 'eval_steps_per_second': 13.461, 'epoch': 0.82}\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/mcwave/anaconda3/envs/torch200/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "{'loss': 1.4727, 'learning_rate': 5e-06, 'epoch': 0.84}\n",
      "{'loss': 1.5012, 'learning_rate': 5e-06, 'epoch': 0.86}\n",
      "{'loss': 1.5086, 'learning_rate': 5e-06, 'epoch': 0.87}\n",
      "{'loss': 1.468, 'learning_rate': 5e-06, 'epoch': 0.89}\n",
      "{'loss': 1.5063, 'learning_rate': 5e-06, 'epoch': 0.91}\n",
      "{'eval_loss': 1.4146875143051147, 'eval_runtime': 1.3065, 'eval_samples_per_second': 153.085, 'eval_steps_per_second': 13.012, 'epoch': 0.91}\n",
      "{'loss': 1.5008, 'learning_rate': 5e-06, 'epoch': 0.92}\n",
      "{'loss': 1.4543, 'learning_rate': 5e-06, 'epoch': 0.94}\n",
      "{'loss': 1.3906, 'learning_rate': 5e-06, 'epoch': 0.96}\n",
      "{'loss': 1.441, 'learning_rate': 5e-06, 'epoch': 0.97}\n",
      "{'loss': 1.4137, 'learning_rate': 5e-06, 'epoch': 0.99}\n",
      "{'eval_loss': 1.3887499570846558, 'eval_runtime': 1.2738, 'eval_samples_per_second': 157.013, 'eval_steps_per_second': 13.346, 'epoch': 0.99}\n",
      "{'loss': 1.5078, 'learning_rate': 5e-06, 'epoch': 1.0}\n",
      "{'loss': 1.1393, 'learning_rate': 5e-06, 'epoch': 1.02}\n",
      "{'loss': 1.2918, 'learning_rate': 5e-06, 'epoch': 1.04}\n",
      "{'loss': 1.3064, 'learning_rate': 5e-06, 'epoch': 1.05}\n",
      "{'loss': 1.2727, 'learning_rate': 5e-06, 'epoch': 1.07}\n",
      "{'eval_loss': 1.4944530725479126, 'eval_runtime': 1.2651, 'eval_samples_per_second': 158.089, 'eval_steps_per_second': 13.438, 'epoch': 1.07}\n",
      "{'loss': 1.1967, 'learning_rate': 5e-06, 'epoch': 1.09}\n",
      "{'loss': 1.2893, 'learning_rate': 5e-06, 'epoch': 1.1}\n",
      "{'loss': 1.2746, 'learning_rate': 5e-06, 'epoch': 1.12}\n",
      "{'loss': 1.2619, 'learning_rate': 5e-06, 'epoch': 1.14}\n",
      "{'loss': 1.2164, 'learning_rate': 5e-06, 'epoch': 1.15}\n",
      "{'eval_loss': 1.4736719131469727, 'eval_runtime': 1.4264, 'eval_samples_per_second': 140.213, 'eval_steps_per_second': 11.918, 'epoch': 1.15}\n",
      "{'loss': 1.2242, 'learning_rate': 5e-06, 'epoch': 1.17}\n",
      "{'loss': 1.1912, 'learning_rate': 5e-06, 'epoch': 1.19}\n",
      "{'loss': 1.2121, 'learning_rate': 5e-06, 'epoch': 1.2}\n",
      "{'loss': 1.2154, 'learning_rate': 5e-06, 'epoch': 1.22}\n",
      "{'loss': 1.2307, 'learning_rate': 5e-06, 'epoch': 1.24}\n",
      "{'eval_loss': 1.4375780820846558, 'eval_runtime': 1.282, 'eval_samples_per_second': 156.006, 'eval_steps_per_second': 13.26, 'epoch': 1.24}\n",
      "{'loss': 1.0318, 'learning_rate': 5e-06, 'epoch': 1.25}\n",
      "{'loss': 1.2982, 'learning_rate': 5e-06, 'epoch': 1.27}\n",
      "{'loss': 1.2283, 'learning_rate': 5e-06, 'epoch': 1.29}\n",
      "{'loss': 1.3264, 'learning_rate': 5e-06, 'epoch': 1.3}\n",
      "{'loss': 1.1914, 'learning_rate': 5e-06, 'epoch': 1.32}\n",
      "{'eval_loss': 1.4385546445846558, 'eval_runtime': 1.269, 'eval_samples_per_second': 157.601, 'eval_steps_per_second': 13.396, 'epoch': 1.32}\n",
      "{'loss': 1.2437, 'learning_rate': 5e-06, 'epoch': 1.33}\n",
      "{'loss': 1.2162, 'learning_rate': 5e-06, 'epoch': 1.35}\n",
      "{'loss': 1.2969, 'learning_rate': 5e-06, 'epoch': 1.37}\n",
      "{'loss': 1.2539, 'learning_rate': 5e-06, 'epoch': 1.38}\n",
      "{'loss': 1.2039, 'learning_rate': 5e-06, 'epoch': 1.4}\n",
      "{'eval_loss': 1.4507031440734863, 'eval_runtime': 1.2533, 'eval_samples_per_second': 159.58, 'eval_steps_per_second': 13.564, 'epoch': 1.4}\n",
      "{'loss': 1.208, 'learning_rate': 5e-06, 'epoch': 1.42}\n",
      "{'loss': 1.2584, 'learning_rate': 5e-06, 'epoch': 1.43}\n",
      "{'loss': 1.2332, 'learning_rate': 5e-06, 'epoch': 1.45}\n",
      "{'loss': 1.1676, 'learning_rate': 5e-06, 'epoch': 1.47}\n",
      "{'loss': 1.3105, 'learning_rate': 5e-06, 'epoch': 1.48}\n",
      "{'eval_loss': 1.5036718845367432, 'eval_runtime': 1.2518, 'eval_samples_per_second': 159.771, 'eval_steps_per_second': 13.58, 'epoch': 1.48}\n",
      "{'loss': 1.2637, 'learning_rate': 5e-06, 'epoch': 1.5}\n",
      "{'loss': 1.2332, 'learning_rate': 5e-06, 'epoch': 1.52}\n",
      "{'loss': 1.3252, 'learning_rate': 5e-06, 'epoch': 1.53}\n",
      "{'loss': 1.2396, 'learning_rate': 5e-06, 'epoch': 1.55}\n",
      "{'loss': 1.135, 'learning_rate': 5e-06, 'epoch': 1.57}\n",
      "{'eval_loss': 1.434999942779541, 'eval_runtime': 1.4342, 'eval_samples_per_second': 139.452, 'eval_steps_per_second': 11.853, 'epoch': 1.57}\n",
      "{'loss': 1.2756, 'learning_rate': 5e-06, 'epoch': 1.58}\n",
      "{'loss': 1.1266, 'learning_rate': 5e-06, 'epoch': 1.6}\n",
      "{'loss': 1.2244, 'learning_rate': 5e-06, 'epoch': 1.61}\n",
      "{'loss': 1.3889, 'learning_rate': 5e-06, 'epoch': 1.63}\n",
      "{'loss': 1.2553, 'learning_rate': 5e-06, 'epoch': 1.65}\n",
      "{'eval_loss': 1.4248827695846558, 'eval_runtime': 1.2594, 'eval_samples_per_second': 158.807, 'eval_steps_per_second': 13.499, 'epoch': 1.65}\n",
      "{'loss': 1.1707, 'learning_rate': 5e-06, 'epoch': 1.66}\n",
      "{'loss': 1.3199, 'learning_rate': 5e-06, 'epoch': 1.68}\n",
      "{'loss': 1.3189, 'learning_rate': 5e-06, 'epoch': 1.7}\n",
      "{'loss': 1.2824, 'learning_rate': 5e-06, 'epoch': 1.71}\n",
      "{'loss': 1.3219, 'learning_rate': 5e-06, 'epoch': 1.73}\n",
      "{'eval_loss': 1.4167969226837158, 'eval_runtime': 1.2648, 'eval_samples_per_second': 158.128, 'eval_steps_per_second': 13.441, 'epoch': 1.73}\n",
      "{'loss': 1.1758, 'learning_rate': 5e-06, 'epoch': 1.75}\n",
      "{'loss': 1.2787, 'learning_rate': 5e-06, 'epoch': 1.76}\n",
      "{'loss': 1.1875, 'learning_rate': 5e-06, 'epoch': 1.78}\n",
      "{'loss': 1.1416, 'learning_rate': 5e-06, 'epoch': 1.8}\n",
      "{'loss': 1.2609, 'learning_rate': 5e-06, 'epoch': 1.81}\n",
      "{'eval_loss': 1.494570255279541, 'eval_runtime': 1.382, 'eval_samples_per_second': 144.721, 'eval_steps_per_second': 12.301, 'epoch': 1.81}\n",
      "{'loss': 1.2805, 'learning_rate': 5e-06, 'epoch': 1.83}\n",
      "{'loss': 1.1885, 'learning_rate': 5e-06, 'epoch': 1.85}\n",
      "{'loss': 1.1684, 'learning_rate': 5e-06, 'epoch': 1.86}\n",
      "{'loss': 1.2318, 'learning_rate': 5e-06, 'epoch': 1.88}\n",
      "{'loss': 1.2465, 'learning_rate': 5e-06, 'epoch': 1.89}\n",
      "{'eval_loss': 1.4358203411102295, 'eval_runtime': 1.3776, 'eval_samples_per_second': 145.178, 'eval_steps_per_second': 12.34, 'epoch': 1.89}\n",
      "{'loss': 1.2871, 'learning_rate': 5e-06, 'epoch': 1.91}\n",
      "{'loss': 1.3854, 'learning_rate': 5e-06, 'epoch': 1.93}\n",
      "{'loss': 1.2385, 'learning_rate': 5e-06, 'epoch': 1.94}\n",
      "{'loss': 1.1982, 'learning_rate': 5e-06, 'epoch': 1.96}\n",
      "{'loss': 1.2197, 'learning_rate': 5e-06, 'epoch': 1.98}\n",
      "{'eval_loss': 1.4189062118530273, 'eval_runtime': 1.2874, 'eval_samples_per_second': 155.352, 'eval_steps_per_second': 13.205, 'epoch': 1.98}\n",
      "{'loss': 1.25, 'learning_rate': 5e-06, 'epoch': 1.99}\n",
      "{'train_runtime': 10391.6848, 'train_samples_per_second': 1.4, 'train_steps_per_second': 0.117, 'train_loss': 1.4608988879736409, 'epoch': 2.0}\n",
      "2023-06-18 01:02:11 INFO [__main__] Saving Model to /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems/\n",
      "2023-06-18 01:02:11 INFO [__main__] Saving Model to /home/mcwave/data/pythia_models/pythia-2.8b_wordproblems/\n",
      "2023-06-18 01:02:11 INFO [__main__] Saving Model to /home/mcwave/data/results/model\n",
      "2023-06-18 01:02:11 INFO [__main__] Done.\n",
      "[2023-06-18 01:02:17,688] [INFO] [launch.py:460:main] Process 12068 exits successfully.\n",
      "2023-06-18 01:02:28 INFO [__main__] Saving Model to /home/mcwave/data/results/model\n",
      "2023-06-18 01:02:44 INFO [__main__] Done.\n",
      "[2023-06-18 01:02:48,413] [INFO] [launch.py:460:main] Process 12067 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "model_name = \"dolly\"\n",
    "\n",
    "model_path = \"/home/mcwave/data/pythia_models/pythia-2.8b_wordproblems\"\n",
    "\n",
    "experiment_id = \"1\"\n",
    "input_model = model_path\n",
    "\n",
    "if experiment_id:\n",
    "    experiment_id = re.sub(r\"\\s+\", \"_\", experiment_id.strip())\n",
    "    model_name = f\"{model_name}__{experiment_id}\"\n",
    "\n",
    "checkpoint_dir_name = \"model\"\n",
    "\n",
    "root_path = '/home/mcwave/data' #os.getcwd()\n",
    "deepspeed_config = os.path.join(root_path, \"config/ds_z3_bf16_config.json\")\n",
    "\n",
    "dolly_training_dir_name = \"dolly_training\"\n",
    "\n",
    "# Use the local training root path if it was provided.  Otherwise try to find a sensible default.\n",
    "local_training_root = model_path\n",
    "if not local_training_root:\n",
    "    # Use preferred path when working in a Databricks cluster if it exists.\n",
    "    if os.path.exists(\"/local_disk0\"):\n",
    "        local_training_root = os.path.join(\"/local_disk0\", dolly_training_dir_name)\n",
    "    # Otherwise use the home directory.\n",
    "    else:\n",
    "        local_training_root = os.path.join(os.path.expanduser('~'), dolly_training_dir_name)\n",
    "\n",
    "dbfs_output_root = \"/home/mcwave/data/results\"\n",
    "if not dbfs_output_root:\n",
    "    dbfs_output_root = f\"/dbfs/{dolly_training_dir_name}\"\n",
    "\n",
    "os.makedirs(local_training_root, exist_ok=True)\n",
    "os.makedirs(dbfs_output_root, exist_ok=True)\n",
    "\n",
    "local_output_dir = os.path.join(local_training_root, \"\")\n",
    "dbfs_output_dir = os.path.join(dbfs_output_root, checkpoint_dir_name)\n",
    "\n",
    "num_gpus_flag = \"\"\n",
    "num_gpus = \"2\"\n",
    "if num_gpus:\n",
    "    num_gpus = int(num_gpus)\n",
    "    num_gpus_flag = f\"--num_gpus={num_gpus}\"\n",
    "\n",
    "tensorboard_display_dir = f\"{local_output_dir}/runs\"\n",
    "\n",
    "print(f\"Local Output Dir: {local_output_dir}\")\n",
    "print(f\"DBFS Output Dir: {dbfs_output_dir}\")\n",
    "print(f\"Tensorboard Display Dir: {tensorboard_display_dir}\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %load_ext tensorboard\n",
    "# MAGIC %tensorboard --logdir '{tensorboard_display_dir}'\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "!deepspeed {num_gpus_flag} \\\n",
    "     --module training.trainer \\\n",
    "     --input-model {input_model} \\\n",
    "     --deepspeed {deepspeed_config} \\\n",
    "     --epochs 2 \\\n",
    "     --local-output-dir {local_output_dir} \\\n",
    "     --dbfs-output-dir {dbfs_output_dir} \\\n",
    "     --per-device-train-batch-size 6 \\\n",
    "     --per-device-eval-batch-size 6 \\\n",
    "     --logging-steps 10 \\\n",
    "     --save-steps 500 \\\n",
    "     --save-total-limit 20 \\\n",
    "     --eval-steps 50 \\\n",
    "     --warmup-steps 50 \\\n",
    "     --test-size 200 \\\n",
    "     --lr 5e-6 \\\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# from training.generate import generate_response, load_model_tokenizer_for_generate\n",
    "\n",
    "# model, tokenizer = load_model_tokenizer_for_generate(input_model)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Examples from https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:/home/mcwave/data/models/falcon-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690c4716871b48c89b73ba1040e08632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from training.generate import generate_response, load_model_tokenizer_for_generate\n",
    "\n",
    "model, tokenizer = load_model_tokenizer_for_generate(input_model+\"checkpoint-9500-0607\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "\n",
      "\n",
      "<|endoftext|>### Disclaimer: This experiment was written by John Tukey and Robert F. Sproull.\n",
      "<|endoftext|>![informs](http://www.realnumbers.org/websites/innumeracy-club/informs.png)<|endoftext|><br>\n",
      "\n",
      "\n",
      "### Announcements:\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "## answer\n",
      "\n",
      "24\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "### Discuss on Hacker News\n",
      "\n",
      "[![Go to Hacker News](https://hacker-news-badge.vercel.app/api?compact=1&header=true&hide_redesign=true)](https://news.ycombinator.com/item?id=31386501)<|endoftext|>---\n",
      "\n",
      "## messages\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "##\n",
      "<|endoftext|>#\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "## instructions\n",
      "\n",
      "24\n",
      "\n",
      "1143540712\n",
      "\n",
      "### Topic stats\n",
      "\n",
      "\n",
      "Solved\n",
      "115.5k\n",
      "---\n",
      "0\n",
      " 0\n",
      "\n",
      "\n",
      "\n",
      "## solution\n",
      "\n",
      "Yes, Natalia sold &nbsp;&nbsp;&nbsp;&nbsp;24&nbsp;&nbsp;&nbsp;&nbsp;clips during April, and she sold &nbsp;&nbsp;&nbsp;&nbsp;12&nbsp;&nbsp;&nbsp;&nbsp;cl);><|endoftext|>#<script async src=\"https://www\n"
     ]
    }
   ],
   "source": [
    "instruction=\"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
    "print(generate_response(instruction, model=model, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Scheduler if mentioned here is a non-user AI bot. Given user command, parse into variables:IR=1 if scheduling, 2 if rescheduling, 3 if cancelling, 4 if asking for other user’s availability, 0 if none. If IR=1, 2, or 4, evaluate: MO=Identify any phrases that indicate number of options requested. VDT=dictionary of the valid dates and time pairs for the request where each key is the dates(like Monday, June 4th) or date ranges(like next week, in two weeks), and each value is a list of times(like 4pm, morning) requested for that date or date range. If key found, set key to 'ND'. If no time specified in user command, set time value to 0:00-23:30. Duration = requested\\/inferred duration of the event in minutes. TZ=the timezone specified using the corresponding IANA ID. ET= purpose of the event(like dinner, meeting). LOC = locations\\/videoconferencing options. Do NOT return any explanations. User command: Love it Jess!! All of it is so true! And speaking of pillows, I left mine in the guest bedroom!🤣Just hold onto it for me until we meet again💕😘 Yes, parting is such sweet sorrow!\n",
      "\n",
      "IR=0, MO=[], VDT={}, Duration=[], TZ=[], ET=[], LOC=[]\n",
      "\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructions = [\n",
    "    \"Write a love letter to Edgar Allan Poe.\",\n",
    "    \"Write a tweet announcing Dolly, a large language model from Databricks.\",\n",
    "    \"I'm selling my Nikon D-750, write a short blurb for my ad.\",\n",
    "    \"Explain to me the difference between nuclear fission and fusion.\",\n",
    "    \"Give me a list of 5 science fiction books I should read next.\",\n",
    "] \n",
    "instructions_2 = [ \n",
    "    \"487 / 81\", \n",
    "    \"342 / 81\", \n",
    "    \"499 / 81\", \n",
    "    \"4295 / 81\", \n",
    "    \"9635 / 3711\"]\n",
    "instructions_3=[\n",
    "    \"123 + 456\",\n",
    "    \"1234 + 5678\",\n",
    "    \"456 - 123\",\n",
    "    \"5678 - 1234\",\n",
    "    \"123 * 456\",\n",
    "    \"1234 * 567\",\n",
    "    \"456 / 123\",\n",
    "    \"5678 / 1234\",\n",
    "    \"9999 * 999\"\n",
    "]\n",
    "\n",
    "message = \"Love it Jess!! All of it is so true! And speaking of pillows, I left mine in the guest bedroom!🤣Just hold onto it for me until we meet again💕😘 Yes, parting is such sweet sorrow!\"\n",
    "instructions_4=[\"Scheduler if mentioned here is a non-user AI bot. Given user command, parse into variables:IR=1 if scheduling, 2 if rescheduling, 3 if cancelling, 4 if asking for other user\\u2019s availability, 0 if none. If IR=1, 2, or 4, evaluate: MO=Identify any phrases that indicate number of options requested. VDT=dictionary of the valid dates and time pairs for the request where each key is the dates(like Monday, June 4th) or date ranges(like next week, in two weeks), and each value is a list of times(like 4pm, morning) requested for that date or date range. If key found, set key to 'ND'. If no time specified in user command, set time value to 0:00-23:30. Duration = requested\\/inferred duration of the event in minutes. TZ=the timezone specified using the corresponding IANA ID. ET= purpose of the event(like dinner, meeting). LOC = locations\\/videoconferencing options. Do NOT return any explanations. User command: \"+message]\n",
    "\n",
    "for instruction in instructions_4:\n",
    "    response = generate_response(instruction, model=model, tokenizer=tokenizer) \n",
    "    if response: \n",
    "        print(f\"Instruction: {instruction}\\n\\n{response}\\n\\n-----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('/home/mcwave/data/scheduling_data/finetune_data_4_dolly.jsonl', 'r') as handle:\n",
    "    json_data = [json.loads(line) for line in handle]\n",
    "\n",
    "out = pd.DataFrame(columns=['message','Dolly_Response'])\n",
    "out.to_csv('/home/mcwave/data/scheduling_output/finetune3_output.csv', mode='w',index=False)\n",
    "\n",
    "for data in json_data:\n",
    "    message = data['instruction'].split('User command: ')[1]\n",
    "    instruction = data['instruction']\n",
    "    response = generate_response(instruction, model=model, tokenizer=tokenizer)\n",
    "    out = pd.DataFrame(zip([message],[response]), columns=['message','Dolly_Response'])\n",
    "    out.to_csv('/home/mcwave/data/scheduling_output/finetune3_output.csv', mode='a',header=False,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9 9 9 * 9 9 9 = < [ { ( 9 * 9 = 81 ) shifted 3 = 81000 } + { ( 9 * 9 = 81 ) shifted 2 = 8100 } + { ( 9 * 9 = 81 ) shifted 1 = 810 } + ( 9 * 9 = 81 ) = 89991 ] shifted 2 = 8999100 > + < [ { ( 9 * 9 = 81 ) shifted 3 = 81000 } + { ( 9 * 9 = 81 ) shifted 2 = 8100 } + { ( 9 * 9 = 81 ) shifted 1 = 810 } + ( 9 * 9 = 81 ) = 89991 ] shifted 1 = 899910 > + [ { ( 9 * 9 = 81 ) shifted 3 = 81000 } + { ( 9 * 9 = 81 ) shifted 2 = 8100 } + { ( 9 * 9 = 81 ) shifted 1 = 810 } + ( 9 * 9 = 81 ) = 89991 ] = 8 9 9 9 1 0 0 + 8 9 9 9 1 0 + 8 9 9 9 1 = 9 8 9 9 0 1 0 + 8 9 9 9 1 = 9 9 8 9 0 0 1 \n"
     ]
    }
   ],
   "source": [
    "print(\"9 9 9 9 * 9 9 9 = \"+to_words_mult(\"9999\",\"999\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_eval_set(x):\n",
    "    res=[]\n",
    "    for i in range(x):\n",
    "        op=random.randint(1,4)\n",
    "        if op==1:\n",
    "            digits_1=random.randint(2,4)\n",
    "            digits_2=random.randint(1,digits_1)\n",
    "            a=random.randint(10**(digits_1-1),(10**digits_1)-1)\n",
    "            b=random.randint(max(10**(digits_2-1),1),min((10**digits_2)-1,a))\n",
    "            res.append(str(a)+\" / \"+str(b))\n",
    "        if op==2:\n",
    "            a=random.randrange(2,9999)\n",
    "            b=random.randrange(2,999)\n",
    "            res.append(str(a)+\" * \"+str(b))\n",
    "        if op==3:\n",
    "            a=random.randrange(2,9999)\n",
    "            b=random.randrange(2,9999)\n",
    "            res.append(str(a)+\" + \"+str(b))\n",
    "        if op==4:\n",
    "            a=random.randrange(3,9999)\n",
    "            b=random.randrange(2,a)\n",
    "            res.append(str(a)+\" - \"+str(b))\n",
    "    return res\n",
    "eval_set=get_eval_set(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ans(s):\n",
    "    number_1=\"\"\n",
    "    i=0\n",
    "    while s[i]!=\" \":\n",
    "        number_1+=s[i]\n",
    "        i+=1\n",
    "    symbol=s[i+1]\n",
    "    is_div=False\n",
    "    if s[i+1]==\"/\":\n",
    "        is_div=True\n",
    "    i+=3\n",
    "    number_2=\"\"\n",
    "    while i<len(s):\n",
    "        number_2+=s[i]\n",
    "        i+=1\n",
    "    res=space(str(math.floor(eval(s))))\n",
    "    if is_div:\n",
    "        res+=\" remainder \"+space(str(int(number_1)%int(number_2)))\n",
    "    return res\n",
    "def eval_model(model,tokenizer,eval_set):\n",
    "    correct=0\n",
    "    correct_cases=[]\n",
    "    incorrect_cases=[[],[],[],[]]\n",
    "    num_checked=0\n",
    "    sym2idx={\"+\":0,\"-\":1,\"*\":2,\"/\":3}\n",
    "    for instruction in eval_set[200:]:\n",
    "        response = generate_response(instruction, model=model, tokenizer=tokenizer)\n",
    "        it=len(response)-1\n",
    "        while response[it]!=\"=\":\n",
    "            it-=1\n",
    "        response_num=response[it+2:len(response)]\n",
    "        if response_num==to_ans(instruction):\n",
    "            correct+=1\n",
    "            correct_cases.append(instruction)\n",
    "        else:\n",
    "            symbol=\"\"\n",
    "            i=0\n",
    "            while instruction[i] not in \"+-*/\":\n",
    "                i+=1\n",
    "            symbol=instruction[i]\n",
    "            incorrect_cases[sym2idx[symbol]].append(instruction)\n",
    "            print(instruction)\n",
    "            print(response)\n",
    "            i=0\n",
    "            number=\"\"\n",
    "            while instruction[i]!=\" \":\n",
    "                number+=instruction[i]\n",
    "                i+=1\n",
    "            print(to_ans(instruction))\n",
    "        if num_checked%10==0:\n",
    "            print(num_checked)\n",
    "        num_checked+=1\n",
    "    return correct_cases,incorrect_cases\n",
    "correct,incorrect=eval_model(model,tokenizer,eval_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch200]",
   "language": "python",
   "name": "conda-env-torch200-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
