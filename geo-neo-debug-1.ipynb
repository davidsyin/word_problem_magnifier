{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0330a445-f7e0-4080-88ea-6fcb7055c9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['a'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['a'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: https://huggingface.co/docs/transformers/v4.17.0/en/tasks/language_modeling\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.add_special_tokens({'bos_token': '[BOS]'})\n",
    "\n",
    "eli5 = datasets.load_dataset(\"/home/mcwave/data/textbooks/eqs_withcoords\", split=\"train[:100]\")\n",
    "eli5 = eli5.train_test_split(test_size=0.1)\n",
    "eli5 = eli5.flatten()\n",
    "\n",
    "eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f50c9a-3f89-4b28-b551-c86b900e57f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a727d345bbfa4754a0120e8fa0a338ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8127f949d114b5795c384257718c387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0391f790b0043e3afc29c04b8c50cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0608e8423734e37a7ed019a92bc70a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"a\"]], padding=True, truncation=True)\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")\n",
    "# for i in tokenized_eli5[\"train\"][\"input_ids\"]:\n",
    "#     print(len(i))\n",
    "\n",
    "block_size = 257\n",
    "# print(\"_____\")\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    } \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, batch_size=1, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7108540d-d9cd-4882-90b1-7b91b2e1c690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b33d226e7a4e9ca17ce4b3b6a22558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186d2df5d3a04062b94cb37de6427c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_padding(val):\n",
    "    # new_ids = []\n",
    "    # for i in val[\"input_ids\"]:\n",
    "    #     if i == 50257:\n",
    "    #         new_ids.append(1)\n",
    "    #     else:\n",
    "    #         new_ids.append(i)\n",
    "    start_pad_tokens = [50257] * 128\n",
    "    end_pad_tokens = [50257] * (128 - len(val[\"input_ids\"]))\n",
    "    val[\"input_ids\"] = start_pad_tokens + [50258] + val[\"input_ids\"] + end_pad_tokens\n",
    "    val[\"attention_mask\"] = [1] * 128 + [1] + val[\"attention_mask\"] + [0] * (128 - len(val[\"attention_mask\"]))\n",
    "    return val\n",
    "lm_dataset[\"train\"] = lm_dataset[\"train\"].map(add_padding)\n",
    "lm_dataset[\"test\"] = lm_dataset[\"test\"].map(add_padding)\n",
    "# for i in range(len(lm_dataset[\"test\"][\"input_ids\"][0])):\n",
    "#     print(lm_dataset[\"test\"][\"input_ids\"][0][i], lm_dataset[\"test\"][\"attention_mask\"][0][i])\n",
    "for i in lm_dataset[\"test\"]:\n",
    "    if len(i[\"input_ids\"]) != 257:\n",
    "        print(\"ids\", len(i[\"input_ids\"]))\n",
    "    if len(i[\"attention_mask\"]) != 257:\n",
    "        print(\"mask\", len(i[\"attention_mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca594bf-d2d7-4647-8084-2c45095bf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "SEQ_LEN = 128\n",
    "RANGE_START = -2\n",
    "RANGE_END = 2\n",
    "EMBED_SIZE = 768\n",
    "LOWER_BOUND = -1000\n",
    "UPPER_BOUND = 1000\n",
    "\n",
    "# # Input:\n",
    "# #    formula: A formula containing \"x\", which will be replaced to numbers between range_start and range_end\n",
    "# # Output:\n",
    "# #    a sequence of embeddings, each has embed_size dimensions, and each dimension is between LOWER_BOUND and UPPER_BOUND\n",
    "# def generate_seq_embed(func,\n",
    "#                        seq_len = SEQ_LEN,\n",
    "#                        range_start = RANGE_START,\n",
    "#                        range_end = RANGE_END,\n",
    "#                        embed_size = EMBED_SIZE):\n",
    "#     start = time.time()\n",
    "#     seq = torch.zeros(seq_len, embed_size)\n",
    "#     step = (range_end - range_start) / (seq_len*embed_size - 1)\n",
    "#     for i in range(seq_len*embed_size):\n",
    "#         x = range_start + i * step\n",
    "#         #print(formula.replace('x', str(x)))\n",
    "#         #y = eval(formula.replace('x', str(x)))\n",
    "#         y = func(x)\n",
    "#         y = max(LOWER_BOUND, min(UPPER_BOUND,y))\n",
    "#         seq[i // embed_size][i % embed_size] = y\n",
    "#     end = time.time()\n",
    "#     return seq\n",
    "\n",
    "# def find_var(eq):\n",
    "#     replace = [\"math\", \"log\", \"exp\", \"sin\", \"cos\", \"tan\", \"sec\", \"arc\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \".\", \"(\", \")\"]\n",
    "#     for i in replace:\n",
    "#         eq = eq.replace(i, \"\")\n",
    "#     eq = eq.replace(\"+\", \"`\").replace(\"-\", \"`\").replace(\"*\", \"`\").replace(\"/\", \"`\").split(\"`\")\n",
    "#     for i in eq:\n",
    "#         if len(i) > 0:\n",
    "#             return i\n",
    "#     return \"\"\n",
    "\n",
    "# train_embeds = []\n",
    "# start_time = time.time()\n",
    "# i = 0\n",
    "# for inputs in lm_dataset[\"train\"][\"input_ids\"]:\n",
    "#     try:\n",
    "#         splitted = tokenizer.decode(inputs).replace('\"', \"\").replace(\"#\", \"\").replace(\" \", \"\")\n",
    "#         splitted = splitted.replace(find_var(splitted), \"x\")\n",
    "#         eqs_embeds = generate_seq_embed(eval(\"lambda x: \" + splitted))\n",
    "#         padded_ids = np.array(torch.tensor([eqs_embeds]))\n",
    "#         train_embeds.append(padded_ids)\n",
    "#     except:\n",
    "#         train_embeds.append(np.array(torch.ones([1, SEQ_LEN, 768])))\n",
    "#     i += 1\n",
    "#     if i % 100 == 0:\n",
    "#         start_time = time.time()\n",
    "# np.save(\"/home/mcwave/data/textbooks/eqs_embeds_small\", np.asarray(train_embeds))\n",
    "# test_embeds = []\n",
    "# start_time = time.time()\n",
    "# i = 0\n",
    "# for inputs in lm_dataset[\"test\"][\"input_ids\"]:\n",
    "#     try:\n",
    "#         splitted = tokenizer.decode(inputs).replace('\"', \"\").replace(\"#\", \"\").replace(\" \", \"\")\n",
    "#         splitted = splitted.replace(find_var(splitted), \"x\")\n",
    "#         eqs_embeds = generate_seq_embed(eval(\"lambda x: \" + splitted))\n",
    "#         padded_ids = np.array(torch.tensor([eqs_embeds]))\n",
    "#         test_embeds.append(padded_ids)\n",
    "#     except:\n",
    "#         test_embeds.append(np.array(torch.ones([1, SEQ_LEN, 768])))\n",
    "#     i += 1\n",
    "#     if i % 100 == 0:\n",
    "#         start_time = time.time()\n",
    "# np.save(\"/home/mcwave/data/textbooks/eqs_embeds_small_test\", np.asarray(test_embeds))\n",
    "# data = np.load(\"/home/mcwave/data/textbooks/eqs_embeds_small.npy\")\n",
    "# data_list = []\n",
    "\n",
    "# size = SEQ_LEN * 768\n",
    "# for i in range(len(lm_dataset[\"train\"])):\n",
    "#     flattened = torch.flatten(torch.Tensor(data[i]))\n",
    "#     if flattened.shape[0] != SEQ_LEN * 768:\n",
    "#         data_list.append(torch.Tensor.tolist(torch.ones([SEQ_LEN * 768])))\n",
    "#     else:\n",
    "#         data_list.append(torch.Tensor.tolist(flattened))\n",
    "#         continue\n",
    "\n",
    "# data_test = np.load(\"/home/mcwave/data/textbooks/eqs_embeds_small_test.npy\")\n",
    "# data_list_test = []\n",
    "# for i in range(len(lm_dataset[\"test\"])):\n",
    "#     flattened = torch.flatten(torch.Tensor(data_test[i]))\n",
    "#     if flattened.shape[0] != SEQ_LEN * 768:\n",
    "#         data_list_test.append(torch.Tensor.tolist(torch.ones([SEQ_LEN * 768])))\n",
    "#     else:\n",
    "#         data_list_test.append(torch.Tensor.tolist(flattened))\n",
    "#         continue\n",
    "# print(\"Loaded data\")\n",
    "\n",
    "# lm_dataset['train'] = lm_dataset['train'].add_column(\"inputs_embeds\", data_list)\n",
    "# lm_dataset['test'] = lm_dataset['test'].add_column(\"inputs_embeds\", data_list_test)\n",
    "# lm_dataset[\"train\"].save_to_disk(\"train_dataset_small.hf\")\n",
    "# lm_dataset[\"test\"].save_to_disk(\"test_dataset_small.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3404d2-36ee-4065-a773-bb88219f0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_dataset[\"test\"] = datasets.load_from_disk(\"test_dataset_2.hf\")\n",
    "lm_dataset[\"test\"] = datasets.load_from_disk(\"train_dataset_small.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5004b9e3-5d1e-42fa-a5a1-79513d7c6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53b9af4-4192-4d55-ba88-f7db91a77d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-12 20:34:43,891] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50259. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    CausalLMOutputWithPast,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "from transformers.models.gpt_neo.configuration_gpt_neo import GPTNeoConfig\n",
    "from transformers.models.gpt_neo.modeling_gpt_neo import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class MyGPTNeoForCausalLM(GPTNeoPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPTNeoModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past_key_values:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"position_ids\": position_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        # if len(inputs_embeds.shape) != 3:\n",
    "        \n",
    "        batch_size = inputs_embeds.shape[0]\n",
    "        inputs_embeds = torch.reshape(inputs_embeds, (batch_size, 128, 768))\n",
    "        self.transformer.wte = nn.Embedding(50259, 768)\n",
    "        eq_ids = input_ids.to(\"cuda\")[:, -129:]\n",
    "        eq_embeds = self.transformer.wte(eq_ids)\n",
    "        inputs_embeds = torch.cat((inputs_embeds.to(\"cuda\"), eq_embeds), 1)\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            None, #input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds.to(\"cuda\"),\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(\"outputs\", transformer_outputs)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        # print(\"hidden\", hidden_states)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        # print(\"logits\", lm_logits)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            # Compute loss in fp32 to match with mesh-tf version\n",
    "            # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n",
    "            lm_logits = lm_logits.to(torch.float32)\n",
    "\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., 128:-1, :].contiguous()\n",
    "            shift_labels = labels[..., 129:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            lm_logits = lm_logits.to(hidden_states.dtype)\n",
    "            loss = loss.to(hidden_states.dtype)\n",
    "        # print(\"loss\", loss)\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(\n",
    "        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n",
    "    ) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n",
    "        [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n",
    "        beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past_key_values\n",
    "        )\n",
    "\n",
    "model = MyGPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488b9b4e-da59-4b63-8ae6-227e14392a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     per_device_train_batch_size=10,\n",
    "#     per_device_eval_batch_size=10, \n",
    "#     num_train_epochs = 100, \n",
    "#     save_steps = 100\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=lm_dataset[\"test\"],\n",
    "#     eval_dataset=lm_dataset[\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f9b53b-97ab-44b3-91b0-d0c01810768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Pipeline,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "import torch\n",
    "from training.generate import generate_response\n",
    "\n",
    "model_path = \"/home/mcwave/code/word_problem_magnifier/results/checkpoint-100\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "model = MyGPTNeoForCausalLM.from_pretrained(\n",
    "    model_path\n",
    ")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2cf57d8-7e4b-4eab-92de-cfdae3ce49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_tokens = model.generate(\n",
    "#     inputs_embeds = torch.ones([1, 257, 768]).to(\"cuda\"),\n",
    "#     do_sample=True,\n",
    "#     temperature=0.9,\n",
    "#     max_length=100,\n",
    "# )\n",
    "# gen_text = tokenizer.batch_decode(gen_tokens)[0]#functional 2518, linear 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf0ce1b-4f61-49bb-9870-044c53dd5727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50258.]], device='cuda:0')\n",
      "tensor([[50258.]], device='cuda:0') {'input_ids': tensor([[50258.]], device='cuda:0'), 'inputs_embeds': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`inputs`: tensor([[50258.]], device='cuda:0')` were passed alongside input_ids which is not allowed.Make sure to either pass tensor([[50258.]], device='cuda:0') or input_ids=...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[BOS]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids)\n\u001b[0;32m----> 8\u001b[0m gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50257\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(gen_tokens)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:1447\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs, model_kwargs)\n\u001b[0;32m-> 1447\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_model_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:543\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_model_inputs\u001b[0;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m inputs_kwarg \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mpop(input_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_kwarg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`inputs`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` were passed alongside \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to either pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m     )\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_kwarg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs_kwarg\n",
      "\u001b[0;31mValueError\u001b[0m: `inputs`: tensor([[50258.]], device='cuda:0')` were passed alongside input_ids which is not allowed.Make sure to either pass tensor([[50258.]], device='cuda:0') or input_ids=..."
     ]
    }
   ],
   "source": [
    "inputs_embeds = torch.ones([1, 128, 768]).to(\"cuda\")\n",
    "# input_ids = tokenizer(prompt, return_tensors = \"pt\").input_ids\n",
    "# print(input_ids.shape)\n",
    "# input_ids = torch.cat((torch.ones(1, 257 - len(input_ids)), input_ids), 1).type(torch.IntTensor).to(\"cuda\")\n",
    "# print(input_ids.shape)\n",
    "input_ids = torch.Tensor([tokenizer(\"[BOS]\")[\"input_ids\"]]).to(\"cuda\")\n",
    "print(input_ids)\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    pad_token_id = 50257,\n",
    "    input_ids = input_ids, \n",
    "    inputs_embeds = inputs_embeds,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "print(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3755878-1d2c-4c44-b0ef-496df5b527c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"[BOS]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
